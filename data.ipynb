{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9fbf2be2",
   "metadata": {},
   "source": [
    "# Data pre-process"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ce4f089-bb5f-4fa5-a16a-0da82c83c362",
   "metadata": {},
   "source": [
    "### Imports and paths initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "id": "2c024d9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports for the whole notebook\n",
    "from xml.etree import ElementTree as ET\n",
    "import csv\n",
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "import argparse\n",
    "import rdflib\n",
    "import matplotlib.pyplot as plt\n",
    "# from gensim.models import KeyedVectors\n",
    "import nltk\n",
    "# from owl2vec_star import owl2vec_star\n",
    "\n",
    "\n",
    "def parse_args(args=None):\n",
    "    parser = argparse.ArgumentParser(\n",
    "        description='Data pre-process',\n",
    "        usage=''\n",
    "    )\n",
    "    parser.add_argument('--data_dir', type=str, default='data')\n",
    "    parser.add_argument('--onto_dir', type=str, default='ontology')\n",
    "    parser.add_argument('--syn_data_dir', type=str, default='syn_data')\n",
    "    parser.add_argument('--onto_to_embed', type=str, default='hpObo_hoom_ordo.owl')\n",
    "    parser.add_argument('--embedding_cfg_path', type=str, default='./embedding.cfg')\n",
    "\n",
    "    return parser.parse_args(args)\n",
    "\n",
    "\n",
    "# set the path to your data folders here\n",
    "args = parse_args(args=['--data_dir', '../persistent/data'])\n",
    "onto_dir_path = os.path.join(args.data_dir, args.onto_dir)\n",
    "syn_data_dir_path = os.path.join(args.data_dir, args.syn_data_dir)\n",
    "\n",
    "if not os.path.exists(onto_dir_path):\n",
    "    raise ValueError(f'You need an existing ontology directory with the XML dataset inside it, please create \\'{onto_dir_path}\\'')\n",
    "\n",
    "if not os.path.exists(syn_data_dir_path):\n",
    "    os.makedirs(syn_data_dir_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "606da578",
   "metadata": {},
   "source": [
    "### Convert XML dataset to CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1344a509",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset: clinical signs and symptoms in rare diseases\n",
    "# http://www.orphadata.org/cgi-bin/index.php (Phenotypes associated with rare disorders)\n",
    "\n",
    "\n",
    "tree = ET.parse(os.path.join(onto_dir_path, 'en_product4.xml'))\n",
    "root = tree.getroot()\n",
    "\n",
    "\n",
    "headers = ['HPODisorderSetStatus_id', 'Disorder_id', 'OrphaCode', 'ExpertLink',\n",
    "           'Name', 'DisorderType_id', 'DisorderType_name', 'DisorderGroup_id',\n",
    "           'DisorderGroup_Name', 'HPODisorderAssociation_id', 'HPO_id',\n",
    "           'HPOId', 'HPOTerm', 'HPOFrequency_id', 'HPOFrequency_Name',\n",
    "           'DiagnosticCriteria_id', 'DiagnosticCriteria_Name', 'Source',\n",
    "           'ValidationStatus', 'Online', 'ValidationDate']\n",
    "\n",
    "\n",
    "def find_value(row_data, source_tag, target_tag_name, field, text=True):\n",
    "    \"\"\"Finds a sub-tag of a source tag and inputs its value into a dictionary\n",
    "        containing the current row's data\n",
    "\n",
    "    Args:\n",
    "        row_data (dict):\n",
    "            The data for the current row associated with the csv fields\n",
    "        source_tag (Element):\n",
    "            XML parent tag to search from\n",
    "        target_tag_name (str):\n",
    "            Name of the sub-tag to find\n",
    "        field (str):\n",
    "            Field in the csv file\n",
    "        text (bool):\n",
    "            Indicates if the value of the tag to retrieve is its inner text \n",
    "            or its id attribute\n",
    "    Returns:\n",
    "        tag (Element):\n",
    "            Returns the found tag\n",
    "    \"\"\"\n",
    "    tag = source_tag.find(target_tag_name)\n",
    "    tag_v = ''\n",
    "\n",
    "    if tag is not None:  # retrieving either the inner text or the id attribute of the tag\n",
    "        if text:\n",
    "            tag_v = tag.text\n",
    "        elif (len(tag.attrib) > 0):\n",
    "            tag_v = tag.attrib['id']\n",
    "    row_data[field] = tag_v if tag_v is not None else ''\n",
    "\n",
    "    return tag\n",
    "\n",
    "\n",
    "with open(os.path.join(onto_dir_path, 'en_product4.csv'), 'w', encoding='utf-8') as fd:\n",
    "    csvwriter = csv.DictWriter(fd, delimiter=',', fieldnames=headers)\n",
    "    csvwriter.writeheader()\n",
    "\n",
    "    # iterating through all the disorders\n",
    "    for status in root.find('HPODisorderSetStatusList').findall('HPODisorderSetStatus'):\n",
    "        row_data = {}\n",
    "        row_data['HPODisorderSetStatus_id'] = status.attrib['id']\n",
    "\n",
    "        disorder_tag = find_value(row_data, status, 'Disorder', 'Disorder_id', text=False)\n",
    "        find_value(row_data, disorder_tag, 'OrphaCode', 'OrphaCode', text=True)\n",
    "        find_value(row_data, disorder_tag, 'ExpertLink', 'ExpertLink', text=True)\n",
    "        find_value(row_data, disorder_tag, 'Name', 'Name', text=True)\n",
    "\n",
    "        disordertype_tag = find_value(row_data, disorder_tag, 'DisorderType', 'DisorderType_id', text=False)\n",
    "        find_value(row_data, disordertype_tag, 'Name', 'DisorderType_name', text=True)\n",
    "        disordergroup_tag = find_value(row_data, disorder_tag, 'DisorderGroup', 'DisorderGroup_id', text=False)\n",
    "        find_value(row_data, disordergroup_tag, 'Name', 'DisorderGroup_Name', text=True)\n",
    "\n",
    "        for field in ['Source', 'ValidationStatus', 'Online', 'ValidationDate']:\n",
    "            find_value(row_data, status, field, field, text=True)\n",
    "\n",
    "        # iterating through all the disorder associations and writing a row for each association\n",
    "        for association in disorder_tag.find('HPODisorderAssociationList').findall('HPODisorderAssociation'):\n",
    "            row_data['HPODisorderAssociation_id'] = association.attrib['id']\n",
    "\n",
    "            hpo_tag = find_value(row_data, association, 'HPO', 'HPO_id', text=False)\n",
    "            find_value(row_data, hpo_tag, 'HPOId', 'HPOId', text=True)\n",
    "            find_value(row_data, hpo_tag, 'HPOTerm', 'HPOTerm', text=True)\n",
    "            hpofrequency_tag = find_value(row_data, association, 'HPOFrequency', 'HPOFrequency_id', text=False)\n",
    "            find_value(row_data, hpofrequency_tag, 'Name', 'HPOFrequency_Name', text=True)\n",
    "\n",
    "            diagnosticcriteria_tag = find_value(row_data, association, 'DiagnosticCriteria', 'DiagnosticCriteria_id', text=False)\n",
    "            find_value(row_data, diagnosticcriteria_tag, 'Name', 'DiagnosticCriteria_Name', text=True)\n",
    "\n",
    "            csvwriter.writerow(row_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ebdae01d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>HPODisorderSetStatus_id</th>\n",
       "      <th>Disorder_id</th>\n",
       "      <th>OrphaCode</th>\n",
       "      <th>ExpertLink</th>\n",
       "      <th>Name</th>\n",
       "      <th>DisorderType_id</th>\n",
       "      <th>DisorderType_name</th>\n",
       "      <th>DisorderGroup_id</th>\n",
       "      <th>DisorderGroup_Name</th>\n",
       "      <th>HPODisorderAssociation_id</th>\n",
       "      <th>...</th>\n",
       "      <th>HPOId</th>\n",
       "      <th>HPOTerm</th>\n",
       "      <th>HPOFrequency_id</th>\n",
       "      <th>HPOFrequency_Name</th>\n",
       "      <th>DiagnosticCriteria_id</th>\n",
       "      <th>DiagnosticCriteria_Name</th>\n",
       "      <th>Source</th>\n",
       "      <th>ValidationStatus</th>\n",
       "      <th>Online</th>\n",
       "      <th>ValidationDate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>58</td>\n",
       "      <td>http://www.orpha.net/consor/cgi-bin/OC_Exp.php...</td>\n",
       "      <td>Alexander disease</td>\n",
       "      <td>21394</td>\n",
       "      <td>Disease</td>\n",
       "      <td>36547</td>\n",
       "      <td>Disorder</td>\n",
       "      <td>327485</td>\n",
       "      <td>...</td>\n",
       "      <td>HP:0000256</td>\n",
       "      <td>Macrocephaly</td>\n",
       "      <td>28412</td>\n",
       "      <td>Very frequent (99-80%)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>y</td>\n",
       "      <td>y</td>\n",
       "      <td>2016-06-01 00:00:00.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>58</td>\n",
       "      <td>http://www.orpha.net/consor/cgi-bin/OC_Exp.php...</td>\n",
       "      <td>Alexander disease</td>\n",
       "      <td>21394</td>\n",
       "      <td>Disease</td>\n",
       "      <td>36547</td>\n",
       "      <td>Disorder</td>\n",
       "      <td>327486</td>\n",
       "      <td>...</td>\n",
       "      <td>HP:0001249</td>\n",
       "      <td>Intellectual disability</td>\n",
       "      <td>28412</td>\n",
       "      <td>Very frequent (99-80%)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>y</td>\n",
       "      <td>y</td>\n",
       "      <td>2016-06-01 00:00:00.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>58</td>\n",
       "      <td>http://www.orpha.net/consor/cgi-bin/OC_Exp.php...</td>\n",
       "      <td>Alexander disease</td>\n",
       "      <td>21394</td>\n",
       "      <td>Disease</td>\n",
       "      <td>36547</td>\n",
       "      <td>Disorder</td>\n",
       "      <td>327487</td>\n",
       "      <td>...</td>\n",
       "      <td>HP:0001250</td>\n",
       "      <td>Seizures</td>\n",
       "      <td>28412</td>\n",
       "      <td>Very frequent (99-80%)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>y</td>\n",
       "      <td>y</td>\n",
       "      <td>2016-06-01 00:00:00.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>58</td>\n",
       "      <td>http://www.orpha.net/consor/cgi-bin/OC_Exp.php...</td>\n",
       "      <td>Alexander disease</td>\n",
       "      <td>21394</td>\n",
       "      <td>Disease</td>\n",
       "      <td>36547</td>\n",
       "      <td>Disorder</td>\n",
       "      <td>327488</td>\n",
       "      <td>...</td>\n",
       "      <td>HP:0001257</td>\n",
       "      <td>Spasticity</td>\n",
       "      <td>28412</td>\n",
       "      <td>Very frequent (99-80%)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>y</td>\n",
       "      <td>y</td>\n",
       "      <td>2016-06-01 00:00:00.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>58</td>\n",
       "      <td>http://www.orpha.net/consor/cgi-bin/OC_Exp.php...</td>\n",
       "      <td>Alexander disease</td>\n",
       "      <td>21394</td>\n",
       "      <td>Disease</td>\n",
       "      <td>36547</td>\n",
       "      <td>Disorder</td>\n",
       "      <td>327489</td>\n",
       "      <td>...</td>\n",
       "      <td>HP:0001274</td>\n",
       "      <td>Agenesis of corpus callosum</td>\n",
       "      <td>28412</td>\n",
       "      <td>Very frequent (99-80%)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>y</td>\n",
       "      <td>y</td>\n",
       "      <td>2016-06-01 00:00:00.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   HPODisorderSetStatus_id  Disorder_id  OrphaCode  \\\n",
       "0                        1            2         58   \n",
       "1                        1            2         58   \n",
       "2                        1            2         58   \n",
       "3                        1            2         58   \n",
       "4                        1            2         58   \n",
       "\n",
       "                                          ExpertLink               Name  \\\n",
       "0  http://www.orpha.net/consor/cgi-bin/OC_Exp.php...  Alexander disease   \n",
       "1  http://www.orpha.net/consor/cgi-bin/OC_Exp.php...  Alexander disease   \n",
       "2  http://www.orpha.net/consor/cgi-bin/OC_Exp.php...  Alexander disease   \n",
       "3  http://www.orpha.net/consor/cgi-bin/OC_Exp.php...  Alexander disease   \n",
       "4  http://www.orpha.net/consor/cgi-bin/OC_Exp.php...  Alexander disease   \n",
       "\n",
       "   DisorderType_id DisorderType_name  DisorderGroup_id DisorderGroup_Name  \\\n",
       "0            21394           Disease             36547           Disorder   \n",
       "1            21394           Disease             36547           Disorder   \n",
       "2            21394           Disease             36547           Disorder   \n",
       "3            21394           Disease             36547           Disorder   \n",
       "4            21394           Disease             36547           Disorder   \n",
       "\n",
       "   HPODisorderAssociation_id  ...       HPOId                      HPOTerm  \\\n",
       "0                     327485  ...  HP:0000256                 Macrocephaly   \n",
       "1                     327486  ...  HP:0001249      Intellectual disability   \n",
       "2                     327487  ...  HP:0001250                     Seizures   \n",
       "3                     327488  ...  HP:0001257                   Spasticity   \n",
       "4                     327489  ...  HP:0001274  Agenesis of corpus callosum   \n",
       "\n",
       "  HPOFrequency_id       HPOFrequency_Name DiagnosticCriteria_id  \\\n",
       "0           28412  Very frequent (99-80%)                   NaN   \n",
       "1           28412  Very frequent (99-80%)                   NaN   \n",
       "2           28412  Very frequent (99-80%)                   NaN   \n",
       "3           28412  Very frequent (99-80%)                   NaN   \n",
       "4           28412  Very frequent (99-80%)                   NaN   \n",
       "\n",
       "   DiagnosticCriteria_Name Source ValidationStatus Online  \\\n",
       "0                      NaN    NaN                y      y   \n",
       "1                      NaN    NaN                y      y   \n",
       "2                      NaN    NaN                y      y   \n",
       "3                      NaN    NaN                y      y   \n",
       "4                      NaN    NaN                y      y   \n",
       "\n",
       "          ValidationDate  \n",
       "0  2016-06-01 00:00:00.0  \n",
       "1  2016-06-01 00:00:00.0  \n",
       "2  2016-06-01 00:00:00.0  \n",
       "3  2016-06-01 00:00:00.0  \n",
       "4  2016-06-01 00:00:00.0  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(os.path.join(onto_dir_path, 'en_product4.csv'))\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6058d15c",
   "metadata": {},
   "source": [
    "### Dataset to triples, entities and relations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "793735e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "freq_assoc = {  # from csv frequency to frequency code\n",
    "    'Obligate (100%)': 'O',\n",
    "    'Very frequent (99-80%)': 'VF',\n",
    "    'Frequent (79-30%)': 'F',\n",
    "    'Occasional (29-5%)': 'OC',\n",
    "    'Very rare (<4-1%)': 'VR',\n",
    "    'Excluded (0%)': 'E'\n",
    "}\n",
    "\n",
    "freq_code_assoc = {  # from frequency code to output class\n",
    "    'O': 'obligate',\n",
    "    'VF': 'very_frequent',\n",
    "    'F': 'frequent',\n",
    "    'OC': 'occasional',\n",
    "    'VR': 'very_rare',\n",
    "    'E': 'excluded'\n",
    "}\n",
    "\n",
    "dc_association = {  # default: exclusion\n",
    "    'Diagnostic criterion': 'diagnostic_criterion',\n",
    "    'Pathognomonic sign': 'pathognomonic_sign',\n",
    "}\n",
    "\n",
    "\n",
    "def get_association_subclass(orpha, freq, hp):\n",
    "    \"\"\"Returns normalized association class\n",
    "\n",
    "    Args:\n",
    "        orpha (str):\n",
    "            The prefixed Orphanet code\n",
    "        freq (str):\n",
    "            The frequency text\n",
    "        hp (str):\n",
    "            The prefixed HPO ID\n",
    "    Returns:\n",
    "        (str):\n",
    "            The orphacode, hpo id and frequency association\n",
    "    \"\"\"\n",
    "    return orpha + '_' + hp + '_FREQ:' + freq_assoc.get(freq)\n",
    "\n",
    "\n",
    "def get_association_name(orpha, freq, hp):\n",
    "    \"\"\"Returns textual description of the association class\n",
    "\n",
    "    Args:\n",
    "        orpha (str):\n",
    "            The prefixed Orphanet code\n",
    "        freq (str):\n",
    "            The frequency text\n",
    "        hp (str):\n",
    "            The prefixed HPO ID\n",
    "    Returns:\n",
    "        (str):\n",
    "            The orphacode, hpo id and frequency association \n",
    "            textual_description_with_underscores\n",
    "    \"\"\"\n",
    "    return get_normalized_string(orpha_entities.get(orpha) + ' and ' +\n",
    "                                 hpo_entities.get(hp) + ' ' +\n",
    "                                 freq_code_assoc.get(freq_assoc.get(freq)) +\n",
    "                                 ' association')\n",
    "\n",
    "\n",
    "def get_normalized_string(s):\n",
    "    \"\"\"Transforms a string to lowercase and replaces all whitespace runs with an underscore\n",
    "\n",
    "    Args:\n",
    "        s (str):\n",
    "            String to normalize\n",
    "    Returns:\n",
    "        (str):\n",
    "            The normalized string\n",
    "    \"\"\"\n",
    "    return re.sub(r\"\\s+\", '_', s.lower())\n",
    "\n",
    "\n",
    "df_dataset = pd.read_csv(os.path.join(onto_dir_path, 'en_product4.csv'), \n",
    "                         dtype='object')\n",
    "df_dataset['OrphaCode'] = df_dataset['OrphaCode'].map(lambda x: 'ORPHA:' + x)\n",
    "\n",
    "# key is id, value is textual_description_with_underscores\n",
    "assoc_entities = {}\n",
    "dc_entities = {'diagnostic_criterion': 'diagnostic_criterion',\n",
    "               'pathognomonic_sign': 'pathognomonic_sign',\n",
    "               'exclusion': 'exclusion'}\n",
    "freq_assoc_entities = {'obligate': 'obligate', 'very_frequent': 'very_frequent',\n",
    "                       'frequent': 'frequent', 'occasional': 'occasional',\n",
    "                       'very_rare': 'very_rare', 'excluded': 'excluded'}\n",
    "hpo_entities = {}\n",
    "orpha_entities = {}\n",
    "\n",
    "has_object_triples = []  # association has_object HPOId\n",
    "has_subject_triples = []  # association has_subject OrphaCode\n",
    "has_frequency_triples = []  # association has_frequency FrequencyAssociation\n",
    "has_diagnostic_criterion_triples = []  # association has_DC_attribute DC\n",
    "\n",
    "\n",
    "# reading the dataset\n",
    "for orpha, freq, hp, dc, \\\n",
    "    orpha_name, hpo_name in zip(df_dataset['OrphaCode'],\n",
    "                                df_dataset['HPOFrequency_Name'],\n",
    "                                df_dataset['HPOId'],\n",
    "                                df_dataset['DiagnosticCriteria_Name'],\n",
    "                                df_dataset['Name'],\n",
    "                                df_dataset['HPOTerm']):\n",
    "    if hp not in hpo_entities:\n",
    "        hpo_entities[hp] = get_normalized_string(hpo_name)\n",
    "    if orpha not in orpha_entities:\n",
    "        orpha_entities[orpha] = get_normalized_string(orpha_name)\n",
    "\n",
    "    ac = get_association_subclass(orpha, freq, hp)\n",
    "    ac_name = get_association_name(orpha, freq, hp)\n",
    "    assoc_entities[ac] = ac_name\n",
    "\n",
    "    has_object_triples.append((ac, 'association_has_object', hp))\n",
    "    has_subject_triples.append((ac, 'association_has_subject', orpha))\n",
    "    has_frequency_triples.append((ac, 'has_frequency', freq_code_assoc.get(freq_assoc.get(freq))))\n",
    "    has_diagnostic_criterion_triples.append((ac, 'has_DC_attribute', dc_association.get(dc, 'exclusion')))\n",
    "\n",
    "\n",
    "# lists corresponding to each output file\n",
    "triples = []\n",
    "triples_names = []\n",
    "entities = []\n",
    "entities_names = []\n",
    "relations = []\n",
    "\n",
    "# subClassOf triples\n",
    "entities_and_parent_class = [\n",
    "    (assoc_entities, 'association'),\n",
    "    (dc_entities, 'diagnostic_criteria'), \n",
    "    (freq_assoc_entities, 'frequency_association'), \n",
    "    (hpo_entities, 'HPO_Id'), \n",
    "    (orpha_entities, 'OrphaCode')\n",
    "]\n",
    "for (entities_dict, parent_class) in entities_and_parent_class:\n",
    "    for k, v in entities_dict.items(): \n",
    "        triples.append((k, 'subClassOf', get_normalized_string(parent_class)))\n",
    "        triples_names.append((v, 'subClassOf', parent_class))\n",
    "\n",
    "# other properties triples\n",
    "triples_and_entities = [\n",
    "    (has_object_triples, hpo_entities), \n",
    "    (has_subject_triples, orpha_entities), \n",
    "    (has_frequency_triples, freq_assoc_entities), \n",
    "    (has_diagnostic_criterion_triples, dc_entities)\n",
    "]\n",
    "for (properties_triples, entities_dict) in triples_and_entities:\n",
    "    for (s, r, o) in properties_triples:\n",
    "        triples.append((s, r, o))\n",
    "        triples_names.append((assoc_entities.get(s), r, entities_dict.get(o)))\n",
    "\n",
    "# parent entities\n",
    "for i, (k, v) in enumerate(entities_and_parent_class):\n",
    "    parent = get_normalized_string(v)\n",
    "    entities.append((i, parent))\n",
    "    entities_names.append((i, parent))\n",
    "# entities\n",
    "parents_count = len(entities)\n",
    "for i, (k, v) in enumerate({**assoc_entities, **dc_entities, **freq_assoc_entities,\n",
    "                            **hpo_entities, **orpha_entities}.items()):\n",
    "    entities.append((i+parents_count, k))\n",
    "    entities_names.append((i+parents_count, v))\n",
    "\n",
    "# relations\n",
    "for i, r in enumerate(['subClassOf', 'association_has_object',\n",
    "                       'association_has_subject', 'has_frequency', \n",
    "                       'has_DC_attribute']):\n",
    "    relations.append((i, r))\n",
    "\n",
    "\n",
    "# writing to the different files\n",
    "lists_and_files = [\n",
    "    (triples, 'triples.txt'), \n",
    "    (triples_names, 'triples_names.txt'),\n",
    "    (entities, 'entities.dict'),\n",
    "    (entities_names, 'entities_names.dict'),\n",
    "    (relations, 'relations.dict')\n",
    "]\n",
    "for (l, n) in lists_and_files:\n",
    "    with open(os.path.join(onto_dir_path, n), 'w') as f:\n",
    "        for t in l:\n",
    "            f.write('\\t'.join(str(e) for e in t) + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8253c004",
   "metadata": {},
   "source": [
    "### Merge ORDO, HP and HOOM ontologies using Protégé"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce210777",
   "metadata": {},
   "source": [
    "https://bioportal.bioontology.org/ontologies/ORDO?p=summary \n",
    "    \n",
    "https://bioportal.bioontology.org/ontologies/HP?p=summary \n",
    "\n",
    "https://bioportal.bioontology.org/ontologies/HOOM?p=summary\n",
    "'HOOM is a module that qualifies the annotation between a clinical entity and phenotypic abnormalities according to a frequency and by integrating the notion of diagnostic criterion.'\n",
    "\n",
    "Using Protégé, merge HP (in OBO format, very important) into HOOM (OWL) and then ORDO (OWL) into the result of the merge, to obtain a merge of the 3 ontologies, export in OWL/XML format, and export in turtle format too"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43e11616-2b13-4504-8b6e-7eaf2c0587ec",
   "metadata": {},
   "source": [
    "### Seen and unseen RDs from the ontology"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e116e381-0d45-474b-95c7-de21b7ef8522",
   "metadata": {},
   "source": [
    "#### Loading the ontology graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "cc68bf84-f316-42fe-af3c-0a84c4422527",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len g: 4162248\n"
     ]
    }
   ],
   "source": [
    "g = rdflib.Graph()\n",
    "g.parse('../persistent/data/ontology/hpObo_hoom_ordo.ttl')\n",
    "\n",
    "print(f'len g: {len(g)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be932593-cd14-4a8f-b1c2-d1e302f7fe9b",
   "metadata": {},
   "source": [
    "#### Get all RDs and their corresponding groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "id": "8317546a-a855-4d1a-a3dc-688301a708ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# iri of group of disorders class <http://www.orpha.net/ORDO/Orphanet_557492>\n",
    "# part of <http://www.orpha.net/ORDO/Orphanet_C021>\n",
    "# Rare hereditary metabolic disease with peripheral neuropathy <http://www.orpha.net/ORDO/Orphanet_207018>\n",
    "# Subtype of disorder <http://www.orpha.net/ORDO/Orphanet_557494>\n",
    "\n",
    "groups_query = \"\"\"\n",
    "SELECT ?rd_g ?rd\n",
    "WHERE {\n",
    "    ?rd_uri rdf:type owl:Class;\n",
    "            <http://www.orpha.net/ORDO/Orphanet_C021> ?rd_g .\n",
    "    BIND (STR(?rd_uri) AS ?rd) .\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "groups_qres = g.query(groups_query)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abc77ed8-1885-4e8f-bd42-f2b2a4c59f31",
   "metadata": {},
   "source": [
    "#### Get RDs seen in Association classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "id": "3f695049-f2e6-45a0-9ebb-345bbfa12d53",
   "metadata": {},
   "outputs": [],
   "source": [
    "rds_query = \"\"\"\n",
    "SELECT DISTINCT ?rd\n",
    "{\n",
    "    ?subject rdfs:subClassOf :Association .\n",
    "    BIND(REPLACE(STR(?subject), \".*Orpha:([0-9]+).*\", \"$1\") AS ?sub) .\n",
    "    BIND(CONCAT(\"http://www.orpha.net/ORDO/Orphanet_\", ?sub) AS ?rd) .\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "rds_qres = g.query(rds_query)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e7ede9a-5e71-4b44-a03b-931a495c8ae1",
   "metadata": {},
   "source": [
    "#### Building a dictionary of RD groups and their RDs seen in Association classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "id": "941eb583-c4ba-43dd-9e6e-bbcb128c0333",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique, seen in Association classes RDs: 4345 4345\n",
      "Number of RD groups: 1692\n",
      "Total number of RDs in groups: 10484\n",
      "Average: 6.196217494089835\n",
      "test list: 10484 4196\n",
      "Number of seen RDs: 3775 (86.88%)\n",
      "Number of unseen RDs: 570 (13.12%)\n"
     ]
    }
   ],
   "source": [
    "rd_list = []\n",
    "rd_groups_dict = {}\n",
    "\n",
    "# set of all RDs\n",
    "for row in rds_qres:\n",
    "    rd_list.append(str(row.rd))\n",
    "rd_set = set(rd_list)\n",
    "\n",
    "# RD groups dictionary\n",
    "for row in groups_qres:\n",
    "    rd = str(row.rd)\n",
    "    rd_g = str(row.rd_g)\n",
    "    if rd in rd_set:\n",
    "        if rd_g not in rd_groups_dict:\n",
    "            rd_groups_dict[rd_g] = []\n",
    "        if rd not in rd_groups_dict[rd_g]:\n",
    "            rd_groups_dict[rd_g].append(rd)\n",
    "\n",
    "print(f'Number of unique, seen in Association classes RDs: {len(rd_list)} {len(rd_set)}\\nNumber of RD groups: {len(rd_groups_dict)}')\n",
    "\n",
    "len_sum = sum(len(dct) for dct in rd_groups_dict.values())\n",
    "\n",
    "print(f'Total number of RDs in groups: {len_sum}\\nAverage: {len_sum/len(rd_groups_dict)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84667fa1-3c29-4039-acbd-d1a032b2cac8",
   "metadata": {},
   "source": [
    "#### Building sets of seen and unseen RDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 444,
   "id": "bd439517-110d-41fb-bd16-39813ec07438",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of seen RDs: 3474 (79.95%)\n",
      "Number of unseen RDs: 871 (20.05%)\n"
     ]
    }
   ],
   "source": [
    "# seen and unseen RDs sets\n",
    "seen_rds = set()\n",
    "unseen_rds = set()\n",
    "\n",
    "\n",
    "def populating_rd_sets(method='no_groups', shuffle=False, unseen_pct=0.2, selected_rds=[]):\n",
    "    \"\"\"Builds sets of seen and unseen RDs for synthetic data generation.\n",
    "       The populated sets are global variables outside this function. \n",
    "\n",
    "    Args:\n",
    "        method (str):\n",
    "            Method to choose to build the sets, one of 'no_groups', 'parts_groups', 'whole_groups', 'selected_rds':\n",
    "            'no_groups' adds a guaranteed percentage (unseen_pct) of all RDs to unseen RDs, regardless of their groups.\n",
    "            'parts_groups' adds a percentage (unseen_pct) of RDs inside groups to unseen RDs, \n",
    "                eventually stopping if unseen_rds exceeds the unseen_pct of the RDs total.\n",
    "            'whole_groups' adds all RDs of each group to unseen RDs until unseen_rds exceeds the unseen_pct of the RDs total.\n",
    "            'selected_rds' only adds the selected rds to unseen RDs.\n",
    "            Defaults to 'no_groups'.\n",
    "        shuffle (bool)\n",
    "            Whether to shuffle the order of the rd groups dictionary. Defaults to False.\n",
    "        rds_pct (float):\n",
    "            The wished ratio of unseen RDs. Defaults to 0.2.\n",
    "        selected_rds (list-like):\n",
    "            The list-like object of the IRIs of unseen RDs, only used if the method is 'selected_rds'.\n",
    "    \"\"\"\n",
    "    \n",
    "    methods = {'no_groups', 'parts_groups', 'whole_groups', 'selected_rds'}\n",
    "    if method not in methods:\n",
    "        raise Exception(f'Unknown method \"{method}\", should be one of {methods}')\n",
    "\n",
    "    total_rds = len(rd_set)\n",
    "\n",
    "    # building the sets\n",
    "    if method == 'no_groups':\n",
    "        indexes = [i for i in range(len(rd_list))]\n",
    "        if shuffle:\n",
    "            random.shuffle(indexes)\n",
    "        for ind in indexes:  # iterating on all RDs\n",
    "            rd = rd_list[ind]\n",
    "            if (rd not in seen_rds) and (rd not in unseen_rds):\n",
    "                if len(unseen_rds) > total_rds*unseen_pct:\n",
    "                    seen_rds.add(rd)\n",
    "                else:\n",
    "                    unseen_rds.add(rd)\n",
    "\n",
    "    elif method == 'parts_groups':\n",
    "        keys = list(rd_groups_dict.keys())\n",
    "        if shuffle:\n",
    "            random.shuffle(keys)\n",
    "        for rd_group in keys:  # iterating on groups\n",
    "            rds = rd_groups_dict[rd_group]\n",
    "            # counting unseen RDs to stop at rds*pct threshold\n",
    "            unseen_cnt = 0\n",
    "            if len(unseen_rds) > total_rds*unseen_pct:\n",
    "                # no more RDs added to unseen RDs if unseen_rds is over the threshold\n",
    "                unseen_cnt = len(unseen_rds)\n",
    "            for rd in rds:\n",
    "                if rd in unseen_rds:\n",
    "                    unseen_cnt += 1\n",
    "                else:\n",
    "                    if rd not in seen_rds:\n",
    "                        if unseen_cnt > len(rds)*unseen_pct:\n",
    "                            seen_rds.add(rd)\n",
    "                        else:\n",
    "                            unseen_rds.add(rd)\n",
    "                            unseen_cnt += 1\n",
    "\n",
    "    elif method == 'whole_groups':  # pct of unseen RDs not guaranteed\n",
    "        keys = list(rd_groups_dict.keys())\n",
    "        if shuffle:\n",
    "            random.shuffle(keys)\n",
    "        for rd_group in keys:  # iterating on groups\n",
    "            rds = rd_groups_dict[rd_group]\n",
    "            if len(unseen_rds) > total_rds*unseen_pct:\n",
    "                for rd in rds:\n",
    "                    if (rd not in seen_rds) and (rd not in unseen_rds):\n",
    "                        seen_rds.add(rd)\n",
    "            else:\n",
    "                for rd in rds:\n",
    "                    if rd not in unseen_rds:\n",
    "                        unseen_rds.add(rd)\n",
    "\n",
    "    elif method == 'selected_rds':\n",
    "        for rd in selected_rds:  # iterating on chosen groups\n",
    "            if rd in rd_set:\n",
    "                if rd not in unseen_rds:\n",
    "                    unseen_rds.add(rd)\n",
    "            else:\n",
    "                raise Exception(f'Unknown RD {rd}')\n",
    "        # adding the non chosen RDs to seen_rds\n",
    "        for rds in rd_groups_dict.values():\n",
    "            for rd in rds:\n",
    "                if (rd not in seen_rds) and (rd not in unseen_rds):\n",
    "                    seen_rds.add(rd)\n",
    "\n",
    "    if method != 'no_groups':\n",
    "        # TODO: allow some of these to be added to unseen_rds?\n",
    "        # adding the RDs that aren't part of a RD group, in the seen set\n",
    "        for rd in rd_list:\n",
    "            if (rd not in seen_rds) and (rd not in unseen_rds):\n",
    "                seen_rds.add(rd)\n",
    "\n",
    "\n",
    "picked = [\n",
    "    \"http://www.orpha.net/ORDO/Orphanet_517\",  # Acute myelomonocytic leukemia\n",
    "    \"http://www.orpha.net/ORDO/Orphanet_90068\",  # Cocaine intoxication\n",
    "    \"http://www.orpha.net/ORDO/Orphanet_420741\"  # RIDDLE syndrome\n",
    "]\n",
    "# populating_rd_sets(method='parts_groups', shuffle=False, unseen_pct=0.2)\n",
    "populating_rd_sets(method='selected_rds', selected_rds=picked)\n",
    "\n",
    "print(f'Number of seen RDs: {len(seen_rds)} ({(len(seen_rds)/len(rd_set)*100):.2f}%)\\nNumber of unseen RDs: {len(unseen_rds)} ({(len(unseen_rds)/len(rd_set)*100):.2f}%)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff4d9077-ef7a-45c5-b5f3-c6361b8a9000",
   "metadata": {},
   "source": [
    "### Synthetic data generation from the ontology"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 454,
   "id": "06240382-5159-4e7d-8e3e-e35997620c5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12 unique rare diseases, 179 unique phenotypes\n",
      "0/600 patients generated\n",
      "600 seen patients generated (173 columns), 0 unseen patients generated (1 columns), writing to files\n",
      "Total RDs: 12, seen RDs: 12, unseen RDs: 0\n"
     ]
    }
   ],
   "source": [
    "frequency_dict = {  # frequency ids + associated probability\n",
    "    28405: 1,  # Obligate (100%)\n",
    "    28412: 0.895,  # Very frequent (99-80%)\n",
    "    28419: 0.545,  # Frequent (79-30%)\n",
    "    28426: 0.17,  # Occasional (29-5%)\n",
    "    28433: 0.025,  # Very rare (<4-1%)\n",
    "    28440: 0  # Excluded (0%)\n",
    "}\n",
    "\n",
    "\n",
    "def get_orpha_iri(orphacode):\n",
    "    return f'http://www.orpha.net/ORDO/Orphanet_{orphacode}'\n",
    "\n",
    "\n",
    "def gen_syn_data(patients_per_rd=10, use_ontology_rds=False, gen_small_file=False, print_every=0, del_col_th=0, selected_rds=[]):\n",
    "    \"\"\"Generates synthetic seen and unseen data from the ontology into files\n",
    "\n",
    "    Args:\n",
    "        patients_per_rd (int):\n",
    "            Number of generated patients per RD. Defaults to 10.\n",
    "        use_ontology_rds (bool):\n",
    "            Whether to use the seen and unseen RD sets obtained from ontology queries. Defaults to False.\n",
    "        gen_small_file (bool):\n",
    "            Whether to generate very small files to debug the model or the full ones.\n",
    "            Defaults to False.\n",
    "        print_every (int):\n",
    "            Each number of patients to print progress during data generation (0 = no print).\n",
    "            Defaults to 0.\n",
    "        selected_rds (list-like):\n",
    "            Hand-picked list of RD names to use for generation.\n",
    "        del_col_th (int):\n",
    "            How few values for a column to be deleted (not inclusive), 0 for no deletion. Defaults to 0.\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(os.path.join(onto_dir_path, 'en_product4.csv'))\n",
    "\n",
    "    if len(selected_rds) > 0:\n",
    "        # only the selected RDs\n",
    "        df = df.loc[df['Name'].isin(selected_rds)]\n",
    "    elif gen_small_file:\n",
    "        # only the first 10 rows\n",
    "        grouped = df.groupby('Name', sort=False)\n",
    "        df = pd.concat([group for name, group in grouped][:10])\n",
    "    grouped = df.groupby('Name', sort=False)\n",
    "\n",
    "    rd_count = grouped.ngroups\n",
    "    unique_hps = df.HPOTerm.unique()\n",
    "    total_hp_count = len(unique_hps)\n",
    "\n",
    "    print(f'{rd_count} unique rare diseases, {total_hp_count} unique phenotypes')\n",
    "\n",
    "    phenotypes = unique_hps.tolist()\n",
    "    phenotypes_dict = {hp: i for i, hp in enumerate(phenotypes)}\n",
    "\n",
    "    seen_patients_data = [['rare_disease'] + phenotypes]\n",
    "    unseen_patients_data = [['rare_disease'] + phenotypes]\n",
    "\n",
    "    distribution_check = {  # value: count of patients with hp + maximum count of patients that could've had the hp\n",
    "        28405: [0, 0],  # Obligate (100%)\n",
    "        28412: [0, 0],  # Very frequent (99-80%)\n",
    "        28419: [0, 0],  # Frequent (79-30%)\n",
    "        28426: [0, 0],  # Occasional (29-5%)\n",
    "        28433: [0, 0],  # Very rare (<4-1%)\n",
    "        28440: [0, 0]  # Excluded (0%)\n",
    "    }\n",
    "\n",
    "    patients_count = 0\n",
    "\n",
    "    if not use_ontology_rds:\n",
    "        vectorize = np.vectorize(get_orpha_iri)\n",
    "        seen_rds = vectorize(df.OrphaCode.unique())\n",
    "        unseen_rds = []\n",
    "\n",
    "    for group_nb, (name, group) in enumerate(grouped):  # for each RD\n",
    "        hp_count = len(group)\n",
    "        # generate patients_per_rd patients for each RD\n",
    "        for patient in range(patients_count, patients_count+patients_per_rd):\n",
    "            temp_hp = []\n",
    "            proba_results = np.random.rand(hp_count)  # generating random floats for probabilities\n",
    "            rd_n = ''\n",
    "            rd_iri = ''\n",
    "            for i, (orphacode, rd_name, hp_name, frequency_id) in enumerate(zip(\n",
    "                                                            group['OrphaCode'],\n",
    "                                                            group['Name'],\n",
    "                                                            group['HPOTerm'],\n",
    "                                                            group['HPOFrequency_id'])):\n",
    "                distribution_check.get(frequency_id)[1] += 1\n",
    "                if rd_n == '':\n",
    "                    rd_n = rd_name\n",
    "                    rd_iri = get_orpha_iri(orphacode)\n",
    "                if (proba_results[i] >= 1 - frequency_dict[frequency_id]):  # comparing generated float and proba\n",
    "                    temp_hp.append(hp_name)\n",
    "                    distribution_check.get(frequency_id)[0] += 1\n",
    "\n",
    "            if len(temp_hp) > 0:\n",
    "                row = np.zeros((total_hp_count,), dtype=int)\n",
    "                for hp in temp_hp:\n",
    "                    row[phenotypes_dict.get(hp)] = 1\n",
    "                if rd_iri in seen_rds:\n",
    "                    seen_patients_data.append(np.concatenate([[rd_n], row]))\n",
    "                elif rd_iri in unseen_rds:\n",
    "                    unseen_patients_data.append(np.concatenate([[rd_n], row]))\n",
    "                else:\n",
    "                    print(f'Unknown RD {rd_n}')\n",
    "                    break  # skipping unknown RD\n",
    "                if print_every > 0:\n",
    "                    if patients_count % print_every == 0:\n",
    "                        print(f'{patients_count}/{patients_per_rd*rd_count} patients generated')\n",
    "\n",
    "                patients_count += 1\n",
    "\n",
    "    if del_col_th > 0:\n",
    "        # remove columns with < del_col_th patients that have this phenotype\n",
    "        seen_col_sum = np.sum(np.array(seen_patients_data)[1:, 1:].astype(int), axis=0)\n",
    "        unseen_col_sum = np.sum(np.array(unseen_patients_data)[1:, 1:].astype(int), axis=0)\n",
    "        seen_mask = [True] + [s >= del_col_th for s in seen_col_sum]\n",
    "        unseen_mask = [True] + [s >= del_col_th for s in unseen_col_sum]\n",
    "        seen_patients_data = np.transpose(np.transpose(seen_patients_data)[seen_mask])\n",
    "        unseen_patients_data = np.transpose(np.transpose(unseen_patients_data)[unseen_mask])\n",
    "        seen_patients_data = pd.DataFrame(seen_patients_data[1:], columns=seen_patients_data[0])\n",
    "        unseen_patients_data = pd.DataFrame(unseen_patients_data[1:], columns=unseen_patients_data[0])\n",
    "    else:\n",
    "        # remove columns with only zeros\n",
    "        seen_patients_data = pd.DataFrame(seen_patients_data[1:], columns=seen_patients_data[0])\n",
    "        seen_patients_data = seen_patients_data.loc[:, (seen_patients_data != '0').any(axis=0)]\n",
    "        unseen_patients_data = pd.DataFrame(unseen_patients_data[1:], columns=unseen_patients_data[0])\n",
    "        unseen_patients_data = unseen_patients_data.loc[:, (unseen_patients_data != '0').any(axis=0)]\n",
    "\n",
    "    print(f'{len(seen_patients_data)} seen patients generated ({len(seen_patients_data.columns)} columns), {len(unseen_patients_data)} unseen patients generated ({len(unseen_patients_data.columns)} columns), writing to files')\n",
    "\n",
    "    # writing the 2 files\n",
    "    seen_patients_data.to_csv(\n",
    "        os.path.join(syn_data_dir_path, ('small_' if gen_small_file else '') + 'syn_patients_data_seen.csv'),\n",
    "        encoding='utf-8', index=False, header=True\n",
    "    )\n",
    "    unseen_patients_data.to_csv(\n",
    "        os.path.join(syn_data_dir_path, ('small_' if gen_small_file else '') + 'syn_patients_data_unseen.csv'),\n",
    "        encoding='utf-8', index=False, header=True\n",
    "    )\n",
    "\n",
    "    print(f'Total RDs: {rd_count}, seen RDs: {int(len(seen_patients_data)/patients_per_rd)}, unseen RDs: {int(len(unseen_patients_data)/patients_per_rd)}')\n",
    "\n",
    "    return distribution_check\n",
    "\n",
    "\n",
    "rds = [\n",
    "    \"Acute myelomonocytic leukemia\",\n",
    "    \"Deafness-lymphedema-leukemia syndrome\",\n",
    "    \"Acute promyelocytic leukemia\",\n",
    "    \"Chronic myeloid leukemia\",\n",
    "    \"Acute monoblastic/monocytic leukemia\",\n",
    "    \"Acute erythroid leukemia\",\n",
    "    \"Unclassified myelodysplastic syndrome\",\n",
    "    \"Adrenomyeloneuropathy\",\n",
    "    \"Primary myelofibrosis\",\n",
    "    \"Multiple myeloma\",\n",
    "    \"Acute panmyelosis with myelofibrosis\",\n",
    "    \"Alpha-thalassemia-myelodysplastic syndrome\"\n",
    "]\n",
    "distributions = gen_syn_data(patients_per_rd=50, use_ontology_rds=False, gen_small_file=False, print_every=1000, del_col_th=10, selected_rds=rds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa7cb223-15a1-49eb-be1b-16a408104071",
   "metadata": {},
   "source": [
    "#### Plotting the obtained HP frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 433,
   "id": "2efc1076-f81d-4e2d-bd27-a749b25ff15c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(28405, [20, 20]), (28412, [282, 330]), (28419, [669, 1220]), (28426, [152, 900]), (28433, [3, 120]), (28440, [0, 10])]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEWCAYAAABrDZDcAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/MnkTPAAAACXBIWXMAAAsTAAALEwEAmpwYAAAqnklEQVR4nO3de7wXVb3/8debDQoo6hHJUBCo0NRUUtBMTRIvpKblXQ8pVpKat/J4Sf15yEtHs/JkmaamhXlNCynpKB5TK2+AoSGKV5Tt5aRYiIJy+/z+WGvD8HXvzXfj/u4L834+Ht/Hnu+amTVrzcyez6yZ+a5RRGBmZuXVpb0LYGZm7cuBwMys5BwIzMxKzoHAzKzkHAjMzErOgcDMrOQcCFqZpLGSft3e5WgJSaMl/aWZ8V+WNFvSO5I+3ZZlKwNJv5R0QXuXozGSzpJ0zYfMY6CkkNS1ifGbSZomaZ6kkz7MsmzVOBC0UD5o/l3SfEmvS7pC0nrtXa4a+wFwQkSsHRF/a+/CtCVJT0v6aiPpJ0uakoe3lHS3pLck/UvSVEl7N5Ffs0G3PUkaLqm+mBYR34uIr9d40acDf4qIXhFxWY2XZY1wIGgBSacCFwOnAesCnwEGAJMkrdGG5Wj0zKqGBgBPdpCytLVfAUc2kv6VPA7g98Ak4KPAR4CTgLfbpHQt0IG3VZP7F4CkujYsSzlFhD9VfIB1gHeAQyrS1wbeAL6av48FbgNuAeYBjwHbFKY/A3glj5sJjMjpXYAzgeeBOcCtwPp53EAggK8BLwMPAH8knaUXy/I4cEAe/iTp4PRWXs4hhel6AxNIB6tHgfOBvzRS5zVznQN4F3g+p8/K9XgCeB/oSgqKDwL/yuUYXshnEHB/rvMk4KfAr/O44UB9xXJnAbu3YL0cldfLm8DZhXzqgLPyvPOAqUB/4HLghxXLnAB8q5F10A9YDAwopG0BLAQ2yJ8A1qtiH9oceA9Yktfrv3L6L3OZ7szlfAT4eGG+5rblusA40j74EnAO0CWPGw38Fbg0r7sL8jb9QV5f/wdcCfQA1gIWAEtz2d4BNiLtz78uLG/nwnaeDYzO6fsAfyPtU7OBsYV5GrZT10bWyb15fbyXl7lpXh9XABNJ+93uuSy353q+CJxUyKNHnuefwAzSiVp9YXwAnyh8/yVwQeH7vsC0XKcHga0r9sX/IO3rc0n/190L4/fP875N2s9GAgcDUyvq+W3gjvY+jjW5b7Z3ATrLJ2/gxU3szL8CbsrDY4FFwEFAt7wTvZiHN8v/JBvlaQeS/+GBk4GHSQeeNYGfF/Js+Ecal/9he5DOUv9aKMMWeUdeM08zGziadJD+NOkguUWe9mbSAXUt4FOkwPSBQFDIu/IfaVbe+fvnsmxMOtDsTTpw75G/98nTPwT8KJftc6SDXbWBoJr1cnUuxzakwLR5Hn8a8Pe83pXH9wa2B15l+QFzA2A+sGET9Z8EnFP4/l/A+Dws4FngD8CXmsqjMO/oynVNOjDNyeXqCtwA3JzHrWxbjgPuAHrl9fEM8LXCshYDJ+Z5e5CCwgRg/TzP74H/amZbjC1sqwF52x1O2p97A0MK826Vt//WpCDzpYrt9IH/nTz+PuDrFetjLrBTzq8nKYifC6wBfAx4AdgrT38R8Odcp/7AdKoMBHl9/gPYgXTicBRp/1uzsC8+SgpE6wNPAcfmcdvncu6Ry7kxKWivSQramxeW+TfgwPY+jjW5X7Z3ATrLBxgFvN7EuIuASXl4LPBwYVwX4DVgF+ATeafbHehWkcdT5NZB/t6XFFC6Fv6RPlYY34t0tjQgf78QuDYPHwr8uSL/nwP/mXf2RcAnC+O+R8sDwVcL388Arq+Y5678T7UJ6WC0VmHcjVQfCKpZL/0K4x8FDsvDM4H9m6jTU8AeefgEYOJKtv3MwvZ8GfhyYXw/UivnedIZ9QPA4CbyGl25rkkHpmsK3/cGnq5yWy4kB4U87hvAfYVlvVwYp7zPFFsbOwIvNrMtxha21XeA31X5//LfwKV5uGE7tSQQjCt836FYj0JZrsvDLwAjC+PGUH0guAI4vyLvmcCuhX1xVGHc94ErC9vh0ibqdAVwYR7ektRaWbOaddceH98jqN6bwAZNXGftm8c3mN0wEBFLgXpSK+A54BTSP9c/JN0saaM86QDgd/lm479IB6olwIZN5DuPdCnhsJx0OOlMsiGvHRryyvn9O+kadh/SQXRZXqRLCi1VnH8AcHDF8nYmrZeNgH9GxLuruLxq1svrheH5pMt1kM4On28i31+RDvDkv9c3U4bfAn0lfYZ0sOxJWvcARER9RJwQER/P5X2XdKbeEk3VobltuQHpzLy4Pl8inZk2KG6nPrnsUwt5/U9Or0aT61PSDpL+JOkNSXOBY3P5VlXl/rVRxTo4i+X7wEas+v48ADi1Iu/+Oc8Gq7p/HSFJpPtJt0bE+y0oV5tyIKjeQ6TLDgcUEyWtDXwB+N9Ccv/C+C6kM8ZXASLixojYmbQDBunmM6Qd+QsRsV7h0z0iXinkGxVlugk4XNKOQHfgT4W87q/Ia+2IOI50jXVxsYyks/aWKpZlNqlFUFzeWhFxEak19G+S1mpiee+SDk7AshuDxQNTNeulKbOBjzcx7tfA/pK2IV27H99UJhExn3Tf50jSP/XNEbGwiWlnk673f6qp7Kood1Fz2/JNUutoQGH6TUiX+hpb3puk+wBbFvJaNyLWbmTapsrS1Pq8kXTJqX9ErEu696BqKtiEyv3rxYp10CsiGp7Meo3m9+f5FPYxUhAt5n1hRd49I+KmKsrY5PqIiIdJrbVdgCNo/kSj3TkQVCki5gLfBX4iaaSkbpIGkq6117Piht5O0gG59XAKKYA8nJ+X3k3SmqSbYw035yD941woaQCApD6S9l9JsSaSDgLnAbfk1gek69WbSvpKLmc3ScMkbR4RS0hnuGMl9ZS0BekSzofxa+CLkvaSVCepe34UsV9EvARMAb4raQ1JOwNfLMz7DNBd0j6SupFudq5ZGL8q66XBNcD5kgYr2VpSb0hn8cBk0na7PSIWrCSvX5Eu0xzI8qeFkPRvkr4r6ROSukjaAPgq6b5GY/4P6NeCp8xWti1vJa2fXnkdfZu0PT4g7x9XA5dK+kgu/8aS9iqUrbekdZsoyw3A7pIOkdRVUm9JQ/K4XsBbEfGepO1JB7/W8igwT9IZknrkfexTkobl8bcC38nboh/pnkjRNNLZeZ2kkcCuhXFXA8fmFo0krZX3xV5VlOsXwNGSRuRtv7GkTxbGjyNdMlwUER3ykeEGDgQtEBHfJzVJf0B6SuAR0lnBiIpm3x2kg8Y/SWeQB0TEItIB7iLSmdnrpEcNv5Pn+THpjOpuSfNIB5IdVlKe90kH9d1JZ2QN6fOAPUmXjV7Ny7qY5QfYE0jN29dJ10uva9GK+GA5ZpOenjiL1OKYTbpR27B/HZHr8hbp2va4wrxzgeNJB+1XSC2E4rPsLV4vBT8iHSTuJm2vX5BumDb4FekGZzVnaw+QbgzWR8TkQvpC0jXwe/IyppMC/+gm8rmX9Kjk65LebGKaZarYlieS1tkLwF9I+8G1zWR5BvAc6cTk7VzuzfKynia1Ml/Il0mKl0eIiJdJ9y9OJW3LaaQb8JC24Xl5G51LWu+tIge8fYEhpAcv3iTtLw0B67uky0EvkrZ15fY8mXTy8S/SZbXxhbynAMeQDtj/JK2b0VWW61HSTfxLSfvG/azYOrue1DLs8D8wVb6ZYdZmJI0l3bwbtbJpa1yOz5H+SQeE/xFWG5KGk25w92vncvQgPRyybUQ8255lWRm3CKyU8mWok0lP6zgIWC0cB0zu6EEA0tMjZqUiaXPSfYvHSU17s1YlaRbpZvmX2rck1fGlITOzkvOlITOzkut0l4Y22GCDGDhwYHsXw8ysU5k6deqbEdHojwc7XSAYOHAgU6ZMae9imJl1KpKa/MW1Lw2ZmZWcA4GZWck5EJiZlVynu0dgZp3PokWLqK+v57333mvvoqz2unfvTr9+/ejWrVvV8zgQmFnN1dfX06tXLwYOHEjqmdlqISKYM2cO9fX1DBo0qOr5fGnIzGruvffeo3fv3g4CNSaJ3r17t7jlVbNAIOlaSf+QNL2J8ZJ0maTnJD0hadtalcXM2p+DQNtYlfVcyxbBL0nv+W3KF4DB+TOG9Go3MzNrYzW7RxARD+QXtzRlf9J7SYPUN/p6kvpGxGu1KpOZdQwDz7xz5RO1wKyL9qlquvr6er75zW8yY8YMli5dyr777ssll1zCjTfeyJQpU/jpT3/aquX6sNZee23eeeedmi+nPW8Wb8yK7xmtz2kfCASSxpBaDWyyyaq8VTFpzZ1vVvdWfAHT2Lmtl5eZNSoiOOCAAzjuuOO44447WLJkCWPGjOHss89myy23bPXlLV68mK5dO8fzOJ3iZnFEXBURQyNiaJ8+1b5n28xsuXvvvZfu3btz9NGp5/G6ujouvfRSrr32WubPn8/s2bMZPnw4gwcP5rvf/S4A7777Lvvssw/bbLMNn/rUp7jlllsAmDp1Krvuuivbbbcde+21F6+9ls5fhw8fzimnnMLQoUO58MILGTBgAEuXLl2WV//+/Vm0aBHPP/88I0eOZLvttmOXXXbh6aefBuDFF19kxx13ZKuttuKcc85ps3XTnuHqFVZ84XQ/VnzptplZq3nyySfZbrvtVkhbZ5112GSTTVi8eDGPPvoo06dPp2fPngwbNox99tmHl156iY022og770xXE+bOncuiRYs48cQTueOOO+jTpw+33HILZ599Ntdem94QunDhwmX9oT322GPcf//9fP7zn+cPf/gDe+21F926dWPMmDFceeWVDB48mEceeYTjjz+ee++9l5NPPpnjjjuOI488kssvv7zN1k17tggmAEfmp4c+A8z1/QEzay977LEHvXv3pkePHhxwwAH85S9/YauttmLSpEmcccYZ/PnPf2bddddl5syZTJ8+nT322IMhQ4ZwwQUXUF+//DXbhx566ArDDa2Im2++mUMPPZR33nmHBx98kIMPPpghQ4bwjW98Y1mL4q9//SuHH344AF/5ylfarO41axFIugkYDmwgqZ700vJuABFxJTCR9CLs54D5+E1RZlZDW2yxBbfddtsKaW+//TYvv/wyXbt2/cBjl5LYdNNNeeyxx5g4cSLnnHMOI0aM4Mtf/jJbbrklDz30UKPLWWuttZYN77fffpx11lm89dZbTJ06ld122413332X9dZbj2nTpjU6f3s8ZluzFkFEHB4RfSOiW0T0i4hfRMSVOQgQyTcj4uMRsVVEuG9pM6uZESNGMH/+fMaNGwfAkiVLOPXUUxk9ejQ9e/Zk0qRJvPXWWyxYsIDx48ez00478eqrr9KzZ09GjRrFaaedxmOPPcZmm23GG2+8sSwQLFq0iCeffLLRZa699toMGzaMk08+mX333Ze6ujrWWWcdBg0axG9+8xsg3cR+/PHHAdhpp524+eabAbjhhhtqvUqW6Ry3tM1stVLt456tSRK/+93vOP744zn//PNZunQpe++9N9/73ve46aab2H777TnwwAOpr69n1KhRDB06lLvuuovTTjuNLl260K1bN6644grWWGMNbrvtNk466STmzp3L4sWLOeWUU5p88ujQQw/l4IMP5r777luWdsMNN3DcccdxwQUXsGjRIg477DC22WYbfvzjH3PEEUdw8cUXs//++7fRmumE7yweOnRorOqLafz4qFn7eOqpp9h8883buxil0dj6ljQ1IoY2Nn2neHzUzMxqx4HAzKzkHAjMzErOgcDMrOQcCMzMSs6BwMys5Pw7AjNre2PXbeX8mn8Ee86cOYwYMQKA119/nbq6Ovr06cOsWbPYaKONmDFjRuuWpxnjx49n0003ZYsttgDg3HPP5XOf+xy77757i/KZNWsW++67L9OnN/rurxZxi8DMVnu9e/dm2rRpTJs2jWOPPZZvfetby7536dL6h8HFixc3OW78+PErBJ7zzjuvxUGgtTkQmFmpLVmyhGOOOYYtt9ySPffckwULFgA02VX0rFmz2G233dh6660ZMWIEL7/8MgCjR4/m2GOPZYcdduD0009vdP4HH3yQCRMmcNpppzFkyBCef/55Ro8evawPpMmTJ/PZz36WbbbZhu2335558+Yxa9YsdtllF7bddlu23XZbHnzwwVZfB740ZGal9uyzz3LTTTdx9dVXc8ghh3D77bczatSoJruKPvHEEznqqKM46qijuPbaaznppJMYP348kN6A9uCDD1JXV8eIESManX+//fZj33335aCDDlqhHAsXLlzWW+mwYcN4++236dGjBx/5yEeYNGkS3bt359lnn+Xwww9nVXtXaIoDgZmV2qBBgxgyZAgA2223HbNmzVqhq+gG77//PgAPPfQQv/3tb4HUVfTpp5++bJqDDz6Yurq6ZudvysyZM+nbty/Dhg0D0rsSIL3Q5oQTTmDatGnU1dXxzDPPfPhKV3AgMLNSW3PNNZcN19XVsWDBApYuXdpsV9FNaeiCelXnb8yll17KhhtuyOOPP87SpUvp3r37h86zku8RmJlVaK6r6M9+9rMrdBW9yy67tGj+Xr16MW/evA/Ms9lmm/Haa68xefJkAObNm8fixYuZO3cuffv2pUuXLlx//fUsWbKk1evrFoEt15qP9LlHVWtOJ9g/muoq+ic/+QlHH300l1xyCX369OG6665r0fyHHXYYxxxzDJdddtkKL8pZY401uOWWWzjxxBNZsGABPXr04J577uH444/nwAMPZNy4cYwcOXKFF9+0FndDvYpWy26oHQisRtwNddtyN9RmZtYiDgRmZiXnQGBmbaKzXYburFZlPTsQmFnNde/enTlz5jgY1FhEMGfOnBY/Yuqnhsys5vr160d9fT1vvPFGexdltde9e3f69evXonkcCMys5rp168agQYPauxjWBF8aMjMrObcIVgOt9fuIWa3/y3Uz6wTcIjAzKzkHAjOzknMgMDMrOQcCM7OScyAwMys5BwIzs5JzIDAzK7maBgJJIyXNlPScpDMbGb+JpD9J+pukJyTtXcvymJnZB9UsEEiqAy4HvgBsARwuaYuKyc4Bbo2ITwOHAT+rVXnMzKxxtWwRbA88FxEvRMRC4GZg/4ppAlgnD68LvFrD8piZWSNqGQg2BmYXvtfntKKxwChJ9cBE4MTGMpI0RtIUSVPce6GZWetq75vFhwO/jIh+wN7A9ZI+UKaIuCoihkbE0D59+rR5Ic3MVme1DASvAP0L3/vltKKvAbcCRMRDQHdggxqWyczMKtQyEEwGBksaJGkN0s3gCRXTvAyMAJC0OSkQ+NqPmVkbqlkgiIjFwAnAXcBTpKeDnpR0nqT98mSnAsdIehy4CRgdfpedmVmbqun7CCJiIukmcDHt3MLwDGCnWpbBzMya1943i83MrJ05EJiZlZwDgZlZyTkQmJmVnAOBmVnJORCYmZWcA4GZWck5EJiZlZwDgZlZyTkQmJmVnAOBmVnJORCYmZWcA4GZWck5EJiZlZwDgZlZyTkQmJmVnAOBmVnJORCYmZWcA4GZWck5EJiZlZwDgZlZyTkQmJmVnAOBmVnJORCYmZWcA4GZWck5EJiZlZwDgZlZyTkQmJmVXFWBQNJWtS6ImZm1j2pbBD+T9Kik4yWtW9MSmZlZm6oqEETELsC/A/2BqZJulLRHTUtmZmZtoup7BBHxLHAOcAawK3CZpKclHVCrwpmZWe1Ve49ga0mXAk8BuwFfjIjN8/Clzcw3UtJMSc9JOrOJaQ6RNEPSk5JuXIU6mJnZh9C1yul+AlwDnBURCxoSI+JVSec0NoOkOuByYA+gHpgsaUJEzChMMxj4DrBTRPxT0kdWsR5mZraKqg0E+wALImIJgKQuQPeImB8R1zcxz/bAcxHxQp7nZmB/YEZhmmOAyyPinwAR8Y9VqIOZmX0I1d4juAfoUfjeM6c1Z2NgduF7fU4r2hTYVNJfJT0saWSV5TEzs1ZSbYuge0S80/AlIt6R1LOVlj8YGA70Ax6QtFVE/Ks4kaQxwBiATTbZpBUWa2ZmDaptEbwraduGL5K2AxY0Mz3AK6THTRv0y2lF9cCEiFgUES8Cz5ACwwoi4qqIGBoRQ/v06VNlkc3MrBrVtghOAX4j6VVAwEeBQ1cyz2RgsKRBpABwGHBExTTjgcOB6yRtQLpU9EKVZTIzs1ZQVSCIiMmSPglslpNmRsSilcyzWNIJwF1AHXBtRDwp6TxgSkRMyOP2lDQDWAKcFhFzVrUyZmbWctW2CACGAQPzPNtKIiLGNTdDREwEJlaknVsYDuDb+WNmZu2gqkAg6Xrg48A00pk7QADNBgIzM+v4qm0RDAW2yGfwZma2Gqn2qaHppBvEZma2mqm2RbABMEPSo8D7DYkRsV9NSmVmZm2m2kAwtpaFMDOz9lPt46P3SxoADI6Ie/KviutqWzQrs4Fn3tlqec26aJ9Wy8tsdVTtU0PHkLp4WJ/09NDGwJXAiNoVzayVjG2ll+qNnds6+Zh1MNXeLP4msBPwNix7SY27jDYzWw1UGwjej4iFDV8kdSX9jsDMzDq5agPB/ZLOAnrkdxX/Bvh97YplZmZtpdpAcCbwBvB34BukbiMafTOZmZl1LtU+NbQUuDp/zMxsNVLtU0Mv0sg9gYj4WKuXyMzM2lRL+hpq0B04mPQoqZmZdXJV3SOIiDmFzysR8d+kF9qbmVknV+2loW0LX7uQWggteZeBmZl1UNUezH9YGF4MzAIOafXSmJlZm6v2qaHP17ogZmbWPqq9NNTsqyQj4ketUxwzM2trLXlqaBgwIX//IvAo8GwtCmVmZm2n2kDQD9g2IuYBSBoL3BkRo2pVMDMzaxvVdjGxIbCw8H1hTjMzs06u2hbBOOBRSb/L378E/KomJTIzszZV7VNDF0r6I7BLTjo6Iv5Wu2KZmVlbqfbSEEBP4O2I+DFQL2lQjcpkZmZtqKpAIOk/gTOA7+SkbsCva1UoMzNrO9W2CL4M7Ae8CxARrwK9alUoMzNrO9UGgoUREeSuqCWtVbsimZlZW6o2ENwq6efAepKOAe7BL6kxM1strPSpIUkCbgE+CbwNbAacGxGTalw2MzNrAysNBBERkiZGxFaAD/5mZquZai8NPSZpWE1LYmZm7aLaXxbvAIySNIv05JBIjYWta1UwMzNrG80GAkmbRMTLwF6rkrmkkcCPgTrgmoi4qInpDgRuA4ZFxJRVWZaZma2albUIxpN6HX1J0u0RcWC1GUuqAy4H9gDqgcmSJkTEjIrpegEnA4+0qORmZtYqVnaPQIXhj7Uw7+2B5yLihYhYCNwM7N/IdOcDFwPvtTB/MzNrBSsLBNHEcDU2BmYXvtfntGUkbQv0j4g7m8tI0hhJUyRNeeONN1pYDDMza87KLg1tI+ltUsugRx6G5TeL11nVBUvqAvwIGL2yaSPiKuAqgKFDh7Y0IJmZWTOaDQQRUfch8n4F6F/43i+nNegFfAq4L/1mjY8CEyTt5xvGZmZtpyXdULfUZGCwpEGS1gAOY/k7j4mIuRGxQUQMjIiBwMOAg4CZWRurWSCIiMXACcBdwFPArRHxpKTzJO1Xq+WamVnLVPuDslUSEROBiRVp5zYx7fBalsXMzBpXy0tDZmbWCTgQmJmVnAOBmVnJORCYmZWcA4GZWck5EJiZlZwDgZlZyTkQmJmVnAOBmVnJORCYmZWcA4GZWck5EJiZlZwDgZlZyTkQmJmVnAOBmVnJORCYmZWcA4GZWck5EJiZlZwDgZlZyTkQmJmVnAOBmVnJORCYmZWcA4GZWck5EJiZlZwDgZlZyTkQmJmVnAOBmVnJORCYmZWcA4GZWck5EJiZlZwDgZlZyTkQmJmVXE0DgaSRkmZKek7SmY2M/7akGZKekPS/kgbUsjxmZvZBNQsEkuqAy4EvAFsAh0vaomKyvwFDI2Jr4Dbg+7Uqj5mZNa6WLYLtgeci4oWIWAjcDOxfnCAi/hQR8/PXh4F+NSyPmZk1opaBYGNgduF7fU5ryteAPzY2QtIYSVMkTXnjjTdasYhmZtYhbhZLGgUMBS5pbHxEXBURQyNiaJ8+fdq2cGZmq7muNcz7FaB/4Xu/nLYCSbsDZwO7RsT7NSyPmZk1opYtgsnAYEmDJK0BHAZMKE4g6dPAz4H9IuIfNSyLmZk1oWaBICIWAycAdwFPAbdGxJOSzpO0X57sEmBt4DeSpkma0ER2ZmZWI7W8NERETAQmVqSdWxjevZbLNzOzlesQN4vNzKz91LRFYGbLDTzzzlbLa1b3I1otL8bObb28rFNyi8DMrOQcCMzMSs6BwMys5BwIzMxKzoHAzKzkHAjMzErOgcDMrOQcCMzMSs6BwMys5BwIzMxKzoHAzKzkHAjMzErOgcDMrOQcCMzMSs6BwMys5BwIzMxKzoHAzKzkHAjMzErOgcDMrOQcCMzMSs6BwMys5BwIzMxKzoHAzKzkHAjMzErOgcDMrOQcCMzMSs6BwMys5BwIzMxKzoHAzKzkurZ3Acyscxt45p2tltes7ke0TkZj57ZOPiXhFoGZWcnVNBBIGilppqTnJJ3ZyPg1Jd2Sxz8iaWAty2NmZh9Us0AgqQ64HPgCsAVwuKQtKib7GvDPiPgEcClwca3KY2Zmjatli2B74LmIeCEiFgI3A/tXTLM/8Ks8fBswQpJqWCYzM6ugiKhNxtJBwMiI+Hr+/hVgh4g4oTDN9DxNff7+fJ7mzYq8xgBj8tfNgJk1KXTLbAC8udKpOpfVsU6wetZrdawTrJ716ih1GhARfRob0SmeGoqIq4Cr2rscRZKmRMTQ9i5Ha1od6wSrZ71WxzrB6lmvzlCnWl4aegXoX/jeL6c1Oo2krsC6wJwalsnMzCrUMhBMBgZLGiRpDeAwYELFNBOAo/LwQcC9UatrVWZm1qiaXRqKiMWSTgDuAuqAayPiSUnnAVMiYgLwC+B6Sc8Bb5GCRWfRoS5VtZLVsU6wetZrdawTrJ716vB1qtnNYjMz6xz8y2Izs5JzIDAzKzkHAkDStZL+kX/X0JC2vqRJkp7Nf/8tp0vSZblbjCckbZvTN5M0NaftmNO6SrpHUs82qMPJkqZLelLSKTltG0kPSfq7pN9LWqeJeYdIeljSNElTJG3f3nXNy5hW+Lwt6ZQW1On8XL5pku6WtFF716mRMvaX9CdJM/J2OzmnV1vHsZJeKayjvXP6TrkeUyQNzmnr5fXQIf7n8zZ4p5nxF0qaXTmNpBPzfj4xP4SCpJ0lXVrrMq+MGulSR9INeVt8rzDdOZK+1G4FbUxElP4DfA7YFpheSPs+cGYePhO4OA/vDfwREPAZ4JGc/iNgZ9JjsrfntBOB0W1Q/k8B04GepAcA7gE+QXpya9c8zVeB85uY/27gC4X63deR6kp62OB1YEAL6rROYfgk4MqOVKecf19g2zzcC3iG1B1LtXUcC/xHI+m/zfXYGfhhTvsBMLzW+2IT5VwDWKvwfShwPfBOM/N8Jq+fdyrSHyadwJ4DfDFvx7uA9dujbhX76PPAx3J9Hwe2Bq7J4yeRHo/vC/y+Pcva2KdDnB20t4h4gPTUUlGx+4tfAV8qpI+L5GFgPUl9gUWkA3FPYJGk9Ug76rjalh6AzUkHtPkRsRi4HzgA2BR4IE8zCTiwifkDaDjrXBd4NQ93lLqOAJ6PiJeosk4R8Xbh61qkOkLHqRMR8VpEPJaH5wFPARtT/XZrSmVdPg70j4j7WqPc1ZK0uaQfknoC2DSn1QGXAKc3N29EPBwRrzWWLdCNXDdgFPDHiKj8/21rjXWpsw/QI7fCugFLgPOA/2y/YjauU/yyuJ1sWNgRXwc2zMMbA7ML09XntMtJB401gW8A/w/4XkQsbYOyTgculNQbWEA6650CPEk68I0HDmbFH/gVnQLcJekHpLOtz+b0jlLXw4Cb8nC1dULShcCRwFzg8zm5o9SpsqwDgU8Dj9CCOgInSDqStL1PjYh/Av9FqssC4Cuk1sA5tSp7kaS1gENIHUoCXAeMzYEO4ARgQkS8plXrVuynpFbBk8BfgTuAvT5UoVtHY/vVDsAbwGOkFtAngC4Nwb8jcYugCpHads0+ZxsRL0fE8IjYEZhPapo/Jel6pa62N61h+Z4i9dx6N/A/wDTS2cdXgeMlTSVdeljYRBbHAd+KiP7At0i/72hueW1W13wdeD/gNzmp2joREWfnOt1AOgA1qT23n6S1gduBU3JLpto6XgF8HBgCvAb8MNdlWkR8JiI+T7pU8VpajG6R9GtJGzaRX2t4jRQEvh4RO0fELxqCQL5PczDwk1XNPCKuj4hPR8Qo0r56GfAFSbdJurSj3ANpEBGnRMSQiPghcD7w/ySdLelWSce0d/mWae9rUx3lAwxkxXsEM4G+ebgvMDMP/xw4vLHpCmm3AIOBC4FdSde2b2jDunwPOL4ibVPg0Tx8HSlYTMzf57L8NyUC3u4odSWdGd/dxLgm61Qx3SYN27Yj1KliWd1I17i//SHruML+W9iWdwPrk4LhgFyfC2u47+2Z198M4FxSR2cN4/Yhta5n5c9S4DnS9fVp+XNeRX6N3kcANgL+kIfvz3n8J7BHreq2knrvCNxV+P4d4DsV+/HYvD2vzWl3AT3bo7yVnw4VPTuYYvcXR5GaoA3pR+anTz4DzI3CtUxJuwKvRsSzpOuYS/Onpk8OSfpI/rsJ6f7AjYW0hptrVwJExNGRzlL2zrO/SjpAAOwGPJuHO0JdD2f5ZSGqrVPD0zLZ/sDTHahODcsSqfX1VET8aBXq2LeQ3ZdJlwiLjiQFjbdqXZcGEXF3RBwK7EI6wbgjP3k1MCLujIiPRsTAiBgIzI+IT0TEklyvIRFxbpWLOp8UaAB6kFrsNf8/a0aTXepI6ka6/Pp9lpcVUvBao+2L2oj2jkQd4UM60LxGuvlUT2ra9gb+l3RQvIf8VALpLOty0hMCfweGFvIR6eZew7Sbk64PPgHsVOM6/Jl0FvY4MCKnnUx6EuUZ4CLyWX8j8+4MTM3zPgJs1xHqSrrJOwdYt5BWbZ1uJx0YnwB+D2zcEerUyHqPnP+0/Nm7BXW8PtfhCdJBp29hXE/gT0C3/H2XPO1UYLM2/v/annSzujK9uaeGvp//F5fmv2ML4z4N/KLw/RTSPYP/AdZsy7pVlHnvvM2eB86uKN/owj52U94WF7dXWSs/7mLCzKzkfGnIzKzkHAjMzErOgcDMrOQcCMzMSs6BwMys5BwIrMOQtEQr9jg6sL3L1B4kjc6/wjVrE+5ryDqSBRExpLER+cdXijbq+6edjSb9BuLVlUy3yiR1jdRBoZlbBNZxSRqY+3cfRzow9pd0mqTJuY/37xamPVvSM5L+IukmSf+R0++TNDQPbyBpVh6uk3RJIa9v5PTheZ7bJD2d+5NXHjdM0oOSHpf0qKRekh6QNKRQjr9I2qaiHnWSfqDUj/4Tkk7M6efm5U+XdFX+tfNBpG6ab8itoh6StpN0v9L7Eu5q+EVxLk/DOxcuUX6fhqTukq5Tep/B3yR9PqePljRB0r3A/0oap0K/+Lmu+7fmNrROor1/0eaPPw0fUkd50/Lnd6T+c5YCn8nj9yS9CFykk5g/kN4lsR3pl5o9Sd1pP0fupx+4j/zrYWADYFYeHgOck4fXJPXeOQgYTuoaoV9exkOkXwCvAbwADMvzrENqUR8F/HdO2xSY0ki9jgNuA7rm7+sX/+bh64EvNlLmbsCDQJ/8/VCW91UzHdgxD1/E8v6UTi1M80ngZaA7qaVRX1j+rsD4PLwu8GJDGf0p18eXhqwjWeHSUL5H8FKk9wZACgR7An/L39cmdQ7XC/hdRMzP802oYll7AlvnM3BIB8LBpJ4+H42I+pzXNFJAmgu8FhGTYfn7DiT9htSj5GmkXkN/2ciydie9GGdxnreh7/zPSzqdFMDWJ3WT8PuKeTcjvXhoUm6Y1AGvKb0voVdEPJSnuxHYNw/vTO7hMyKeltTwHgeASQ3Lj4j7Jf1MUh/SOw9uD18uKiUHAuvo3i0MC/iviPh5cQLlV3M2YTHLL4F2r8jrxIi4qyKv4cD7haQlNPN/EhHzJU0idWx3CKl1slKSugM/I535z5Y0tqJ8xXI+Gal77OL861WznEa8W/F9HOnlLocBR69intbJ+R6BdSZ3AV9V6r8fSRvnnjofAL6Ur6f3Ir1ZrMEslh+cD6rI67jcMySSNlV6qUpTZgJ9JQ3L0/eS1BAgriH1iz850othKk0CvtEwvaT1WX7QfzPXp1i2eaRWTsNy+2j5e5S7SdoyIv4FzJO0Q57usML8fwb+vaFepG64ZzZRr1+SOkUjImY0U39bjblFYJ1GRNwtaXPgoXyZ5B1gVEQ8JukWUu+p/yB1CdzgB8CtksYAdxbSryFd8nks3wx+g+WvI21s2QslHQr8RFIP0tu/dif1oDlV0tuk9wU05hrSpZknJC0Cro6In0q6mnSd//WKMv8SuFLSAlI/9wcBl0lal/Q/+9+ky0hfA66WtJTUJ//cPP/PgCsk/Z3UIhodEe+rkTeCRcT/SXqK9DY0Kyn3PmqrnXyZ5Z2I+EEbLW8j0g3eT0YbPt4qae2IeCcPn0nqhvrkFubRk3SjfduImLuy6W315EtDZh+C0vuCHyH1P9/Wv3HYJz86Op30voELWjKzpN2Bp4CfOAiUm1sEZmYl5xaBmVnJORCYmZWcA4GZWck5EJiZlZwDgZlZyf1/7t93uARF0MoAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def show_distributions(frequency_obs, frequency_th):\n",
    "\n",
    "    index = np.arange(len(frequency_obs))\n",
    "    bar_width = 0.35\n",
    "\n",
    "    fix, ax = plt.subplots()\n",
    "\n",
    "    ax.bar(index, frequency_obs, bar_width, label='Observed')\n",
    "    ax.bar(index+bar_width, frequency_th, bar_width, label='Theoretical')\n",
    "\n",
    "    ax.set_xlabel('Frequency category')\n",
    "    ax.set_ylabel('Frequency')\n",
    "    ax.set_title('Observed frequency VS theoretical frequency')\n",
    "    ax.set_xticks(index + bar_width / 2)\n",
    "    ax.set_xticklabels(['100%', '99-80%', '79-30%', '29-5%', '<4-1%', '0%'])\n",
    "    ax.legend()\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "sorted_items = sorted(distributions.items(), key=lambda x: x[0])\n",
    "print(sorted_items)\n",
    "frequency_obs = [t[1][0]/t[1][1] if t[1][0] != 0 else t[1][0] for t in sorted_items]\n",
    "sorted_freq = sorted(frequency_dict.items(), key=lambda x: x[0])\n",
    "frequency_th = [x[1] for x in sorted_freq]\n",
    "show_distributions(frequency_obs, frequency_th)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afdfde1c",
   "metadata": {},
   "source": [
    "### ORDO and HPO URIs dictionaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 434,
   "id": "a01ce23c-0768-4088-bafe-c6b58d96bfc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalized names to ORDO URIs and HPO URIs\n",
    "\n",
    "df = pd.read_csv(os.path.join(onto_dir_path, 'en_product4.csv'))\n",
    "\n",
    "df_hp = df[['HPOTerm', 'HPOId']].drop_duplicates()\n",
    "df_rd = df[['Name', 'OrphaCode']].drop_duplicates()\n",
    "df_hp['HPOId'] = 'http://purl.obolibrary.org/obo/' + df_hp['HPOId'].str.replace(':', '_')\n",
    "df_rd['OrphaCode'] = 'http://www.orpha.net/ORDO/Orphanet_' + df_rd['OrphaCode'].astype(str)\n",
    "\n",
    "df_hp.to_csv(os.path.join(onto_dir_path, 'HPO.dict'), sep=';', encoding='utf-8', index=False, header=False)\n",
    "df_rd.to_csv(os.path.join(onto_dir_path, 'ORDO.dict'), sep=';', encoding='utf-8', index=False, header=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "924d4ece",
   "metadata": {},
   "source": [
    "### Embedding the ontology with Owl2Vec*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "1d6bf219-f3cf-4cb2-9d9b-38de47d1606f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ed47500c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"OWL2Vec-Star/OWL2Vec_Standalone.py\", line 6, in <module>\n",
      "    import gensim\n",
      "ModuleNotFoundError: No module named 'gensim'\n"
     ]
    }
   ],
   "source": [
    "output_folder = '../persistent/data/ontology/embeddings/hpObo_hoom_ordo'\n",
    "if not os.path.exists(output_folder):\n",
    "    os.makedirs(output_folder)\n",
    "\n",
    "\"\"\"\n",
    "Parameters:\n",
    "    ontology_file\n",
    "    config_file\n",
    "    uri_doc\n",
    "    lit_doc\n",
    "    mix_doc\n",
    "    -> modify the cfg file for more params (cache dir, epochs, etc.)\n",
    "\"\"\"\n",
    "gensim_model = owl2vec_star.extract_owl2vec_model(None, \"./embedding.cfg\", True, True, True)\n",
    "\n",
    "# Gensim format\n",
    "gensim_model.save(os.path.join(output_folder, 'ontology.embeddings'))\n",
    "\n",
    "# Text format (not required)\n",
    "gensim_model.wv.save_word2vec_format(os.path.join(output_folder, \"ontology.embeddings.txt\"), binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "471bcc2c-22cb-494b-96a8-c15c60787a82",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Embedding vectors generated above\n",
    "model = KeyedVectors.load(output_folder+\"ontology.embeddings\", mmap='r')\n",
    "wv = model.wv\n",
    "\n",
    "word = 'http://www.orpha.net/ORDO/Orphanet_556985'\n",
    "vector = wv[word]  # Get numpy vector of a word\n",
    "print(f\"Vector for {word}: {vector}\")\n",
    "\n",
    "#Most similar cosine similarity\n",
    "result1 = wv.most_similar(positive=[word])\n",
    "print(result1)\n",
    "\n",
    "#Most similar entities: cosmul\n",
    "result2 = wv.most_similar_cosmul(positive=[word])\n",
    "print(result2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:root] *",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
