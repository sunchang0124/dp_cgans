{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ccfaa995",
   "metadata": {},
   "source": [
    "## Code from OntoZSL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b321be9-7d06-4a19-9a70-1c8339bb5e50",
   "metadata": {},
   "source": [
    "### Imports and paths initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "54a9781c-c900-4e80-8864-00b9b54d7e34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import os\n",
    "import re\n",
    "import codecs\n",
    "import argparse\n",
    "import pickle as pkl\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "\n",
    "def parse_args(args=None):\n",
    "    parser = argparse.ArgumentParser(\n",
    "        description='Data pre-process',\n",
    "        usage=''\n",
    "    )\n",
    "    parser.add_argument('--data_dir', type=str, default='data')\n",
    "    parser.add_argument('--onto_dir', type=str, default='ontology')\n",
    "    parser.add_argument('--glove_dir', type=str, default='glove')\n",
    "    parser.add_argument('--struct_embeds_fn', default='Onto_TransE.pkl', help='')\n",
    "    parser.add_argument('--word_embeds_fn', default='Onto_Text_Embed.pkl', help='')\n",
    "    parser.add_argument('--triples_fn', default='triples_names_htr.txt', help='')\n",
    "    parser.add_argument('--entities_fn', default='entities.dict', help='')\n",
    "    parser.add_argument('--entities_names_fn', default='entities_names.dict', help='')\n",
    "    parser.add_argument('--entities_embed_fn', default='entity_500.npy', help='entities embedding filename')\n",
    "    parser.add_argument('--relations_embed_fn', default='relation_500.npy', help='relations embedding filename')\n",
    "    parser.add_argument('--struct_embed_size', default=100, type=int, help='entity structural embeddings size')\n",
    "    parser.add_argument('--text_embed_size', default=300, type=int, help='entity textual embeddings size')\n",
    "    parser.add_argument('--mapping_size', default=100, type=int, help='hidden layer size')\n",
    "    parser.add_argument('--dropout_ratio', default=0.5, type=float, help='')\n",
    "    parser.add_argument('--margin', default=10, type=int, help='')\n",
    "    parser.add_argument('--training_epochs', default=10, type=int, help='')\n",
    "    parser.add_argument('--batch_size', default=100, type=int, help='')\n",
    "    parser.add_argument('--display_loss_step', default=1000, type=int, help='')\n",
    "    parser.add_argument('--initial_learning_rate', default=0.001, type=float, help='')\n",
    "    parser.add_argument('--activation_function', default='', help='')\n",
    "    parser.add_argument('--warm_up_steps', default=0, type=int)\n",
    "\n",
    "    return parser.parse_args(args)\n",
    "\n",
    "\n",
    "# set the path to your data folders here\n",
    "param = parse_args(args=['--data_dir', '../persistent/data',\n",
    "                         '--entities_embed_fn', 'entity_500.npy',\n",
    "                         '--relations_embed_fn', 'relation_500.npy'])\n",
    "onto_dir_path = os.path.join(param.data_dir, param.onto_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03a31cef",
   "metadata": {},
   "source": [
    "### Combining the entities and relations embeddings into one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "89873418",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "124749\n",
      "5\n",
      "(124749, 100)\n",
      "(5, 100)\n",
      "124597\n"
     ]
    }
   ],
   "source": [
    "def loadDict(file_name):\n",
    "    entities = list()\n",
    "    wnids = open(file_name, 'r')\n",
    "    try:\n",
    "        for line in wnids:\n",
    "            line = line[:-1]\n",
    "            index, cls = line.split('\\t')\n",
    "            entities.append(cls)\n",
    "    finally:\n",
    "        wnids.close()\n",
    "    print(len(entities))\n",
    "    return entities\n",
    "\n",
    "\n",
    "entity_file = os.path.join(onto_dir_path, 'entities_names.dict')\n",
    "relation_file = os.path.join(onto_dir_path, 'relations.dict')\n",
    "\n",
    "\n",
    "# load entity dict\n",
    "entities = loadDict(entity_file)\n",
    "relations = loadDict(relation_file)\n",
    "\n",
    "embed_dir = os.path.join(onto_dir_path, 'save_onto_embeds')\n",
    "\n",
    "ent_embed_file = os.path.join(embed_dir, param.entities_embed_fn)\n",
    "rel_embed_file = os.path.join(embed_dir, param.relations_embed_fn)\n",
    "\n",
    "ent_embeds = np.load(ent_embed_file)\n",
    "print(ent_embeds.shape)\n",
    "\n",
    "rel_embeds = np.load(rel_embed_file)\n",
    "print(rel_embeds.shape)\n",
    "\n",
    "embed_dict = dict()\n",
    "for i, ent in enumerate(entities):\n",
    "    embed_dict[ent] = ent_embeds[i].astype('float32')\n",
    "for i, rel in enumerate(relations):\n",
    "    embed_dict[rel] = rel_embeds[i].astype('float32')\n",
    "\n",
    "print(len(embed_dict.keys()))\n",
    "\n",
    "\n",
    "embeddings_path = os.path.join(onto_dir_path, 'embeddings')\n",
    "if not os.path.exists(embeddings_path):\n",
    "    os.makedirs(embeddings_path)\n",
    "\n",
    "with open(os.path.join(onto_dir_path, 'embeddings', 'Onto_TransE.pkl'), 'wb') as f:\n",
    "    pkl.dump(embed_dict, f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48a923ee",
   "metadata": {},
   "source": [
    "### Getting the textual embedding for each entity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bb86e6a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load glove word embedding\n",
      "loaded to dict!\n",
      "124749\n",
      "124749\n",
      "does not have semantic embedding:  761 has:  123831\n",
      "123831\n"
     ]
    }
   ],
   "source": [
    "def get_embedding(entity_str, word_vectors, get_vector):\n",
    "    try:\n",
    "        feat = get_vector(word_vectors, entity_str)\n",
    "        return feat\n",
    "    except:\n",
    "        feat = np.zeros(WORD_VEC_LEN)\n",
    "\n",
    "    str_set = filter(None, re.split(\"[ \\-_]+\", entity_str))\n",
    "    str_set = list(str_set)\n",
    "    cnt_word = 0\n",
    "    for i in range(len(str_set)):\n",
    "        temp_str = str_set[i]\n",
    "        try:\n",
    "            now_feat = get_vector(word_vectors, temp_str)\n",
    "            feat = feat + now_feat\n",
    "            cnt_word = cnt_word + 1\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "    if cnt_word > 0:\n",
    "        feat = feat / cnt_word\n",
    "    return feat\n",
    "\n",
    "\n",
    "def generate_text_embedding(ent2doc):\n",
    "\n",
    "    # all_feats = list()\n",
    "\n",
    "    has = 0\n",
    "    cnt_missed = 0\n",
    "    missed_list = []\n",
    "    entities2vec = dict()\n",
    "    for ent, doc in ent2doc.items():\n",
    "        feat = np.zeros(shape=WORD_VEC_LEN, dtype='float32')\n",
    "\n",
    "        doc = doc.replace('_', ' ')\n",
    "        doc = doc.replace('-', ' ')\n",
    "        # print(doc)\n",
    "\n",
    "        options = doc.split()\n",
    "        cnt_word = 0\n",
    "\n",
    "        for option in options:\n",
    "            now_feat = get_embedding(option.strip(), word_vectors, get_vector)\n",
    "            if np.abs(now_feat.sum()) > 0:\n",
    "                cnt_word += 1\n",
    "                feat += now_feat\n",
    "        if cnt_word > 0:\n",
    "            feat = feat / cnt_word\n",
    "\n",
    "        # if cnt_word != len(options):\n",
    "            # print(ent, 'count:', cnt_word)\n",
    "\n",
    "        if np.abs(feat.sum()) == 0:\n",
    "            # print('cannot find word ' + class_name)\n",
    "            cnt_missed = cnt_missed + 1\n",
    "            missed_list.append(ent + \"###\" + doc)\n",
    "            feat = feat\n",
    "\n",
    "        else:\n",
    "            has += 1\n",
    "            feat = feat / (np.linalg.norm(feat) + 1e-6)\n",
    "\n",
    "        # all_feats.append(feat)\n",
    "            entities2vec[ent] = feat\n",
    "\n",
    "    # all_feats = np.array(all_feats)\n",
    "    # print(all_feats.shape)\n",
    "    # for each in missed_list:\n",
    "        # print(each)\n",
    "    print('does not have semantic embedding: ', cnt_missed, 'has: ', has)\n",
    "\n",
    "    return entities2vec\n",
    "    # entities2vec = dict()\n",
    "    # for i, ent in enumerate(ent_list):\n",
    "    #     entities2vec[ent] = ent_matrix[i]\n",
    "    #     # print(ent_matrix[i])\n",
    "    #\n",
    "    # return entities2vec\n",
    "\n",
    "\n",
    "def glove_google(word_vectors, word):\n",
    "    return word_vectors[word]\n",
    "\n",
    "\n",
    "def get_glove_dict(txt_dir):\n",
    "    print('load glove word embedding')\n",
    "    txt_file = os.path.join(txt_dir, 'glove.6B.300d.txt')\n",
    "    word_dict = {}\n",
    "    feat = np.zeros(WORD_VEC_LEN)\n",
    "    with open(txt_file) as fp:\n",
    "        for line in fp:\n",
    "            words = line.split()\n",
    "            assert len(words) - 1 == WORD_VEC_LEN\n",
    "            for i in range(WORD_VEC_LEN):\n",
    "                feat[i] = float(words[i+1])\n",
    "            feat = np.array(feat)\n",
    "            word_dict[words[0]] = feat\n",
    "    print('loaded to dict!')\n",
    "    return word_dict\n",
    "\n",
    "\n",
    "def readTxt(file_name):\n",
    "    class_list = list()\n",
    "    wnids = open(file_name, 'r')\n",
    "    try:\n",
    "        for line in wnids:\n",
    "            line = line[:-1]\n",
    "            class_list.append(line)\n",
    "    finally:\n",
    "        wnids.close()\n",
    "    print(len(class_list))\n",
    "    return class_list\n",
    "\n",
    "\n",
    "def loadDict(file_name):\n",
    "    entities = list()\n",
    "    wnids = open(file_name, 'r')\n",
    "    try:\n",
    "        for line in wnids:\n",
    "            line = line[:-1]\n",
    "            index, cls = line.split('\\t')\n",
    "            entities.append(cls)\n",
    "    finally:\n",
    "        wnids.close()\n",
    "    print(len(entities))\n",
    "    return entities\n",
    "\n",
    "\n",
    "def load_domain_range(triples_file):\n",
    "    text_file = codecs.open(triples_file, \"r\", \"utf-8\")\n",
    "    lines = text_file.readlines()\n",
    "    triples = list()\n",
    "    for line in lines:\n",
    "        line_arr = line.rstrip(\"\\r\\n\").split(\"\\t\")\n",
    "        head = line_arr[0]\n",
    "        rel = line_arr[1]\n",
    "        tail = line_arr[2]\n",
    "        triples.append((head, rel, tail))\n",
    "    return triples\n",
    "\n",
    "\n",
    "WORD_VEC_LEN = 300\n",
    "word_vectors = get_glove_dict(os.path.join(param.data_dir, param.glove_dir))\n",
    "get_vector = glove_google\n",
    "\n",
    "entity_text_file = os.path.join(onto_dir_path, 'entities_names.dict')\n",
    "entity_file = os.path.join(onto_dir_path, 'entities.dict')\n",
    "triples_file = os.path.join(onto_dir_path, 'triples_names.txt')\n",
    "\n",
    "entities = loadDict(entity_file)\n",
    "entities_names = loadDict(entity_text_file)\n",
    "\n",
    "ent2doc = dict()\n",
    "with open(entity_text_file) as f_doc:\n",
    "    lines = f_doc.readlines()\n",
    "\n",
    "    for i in range(len(lines)):\n",
    "        entity_text = lines[i].strip().split('\\t')[1].strip()\n",
    "        ent2doc[entity_text] = entity_text\n",
    "\n",
    "\n",
    "entities2vec = generate_text_embedding(ent2doc)\n",
    "print(len(entities2vec.keys()))\n",
    "\n",
    "\n",
    "with open(os.path.join(onto_dir_path, 'embeddings', 'Onto_Text_Embed.pkl'), 'wb') as f:\n",
    "    pkl.dump(entities2vec, f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9c640bc",
   "metadata": {},
   "source": [
    "### Processing triples (hrt to htr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "addd516a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote (h, t, r) triples to ../persistent/data/ontology/triples_names_htr.txt\n"
     ]
    }
   ],
   "source": [
    "triples_file = os.path.join(onto_dir_path, 'triples_names.txt')\n",
    "save_file = os.path.join(onto_dir_path, 'triples_names_htr.txt')\n",
    "\n",
    "wr_fp = open(save_file, 'w')\n",
    "text_file = codecs.open(triples_file, \"r\", \"utf-8\")\n",
    "lines = text_file.readlines()\n",
    "for line in lines:\n",
    "    line_arr = line.rstrip(\"\\r\\n\").split(\"\\t\")\n",
    "    head = line_arr[0]\n",
    "    rel = line_arr[1]\n",
    "    tail = line_arr[2]\n",
    "\n",
    "    wr_fp.write('%s\\t%s\\t%s\\n' % (head, tail, rel))\n",
    "\n",
    "wr_fp.close()\n",
    "\n",
    "print(f'Wrote (h, t, r) triples to {save_file}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a9b11fa",
   "metadata": {},
   "source": [
    "### Model training for text-aware embedding (PyTorch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "690a5f67",
   "metadata": {},
   "source": [
    "#### Model and DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "2b0f0c7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainDataSet(Dataset):\n",
    "\n",
    "    def __init__(self, data_dir, onto_dir, struct_embeds_fn, word_embeds_fn, triples_fn, entities_fn, entities_names_fn):\n",
    "        data_path = os.path.join(data_dir, onto_dir)\n",
    "\n",
    "        with open(os.path.join(data_path, 'embeddings', struct_embeds_fn), 'rb') as f:\n",
    "            self.struct_embeds = pkl.load(f)\n",
    "        with open(os.path.join(data_path, 'embeddings', word_embeds_fn), 'rb') as f:\n",
    "            self.word_embeds = pkl.load(f)\n",
    "\n",
    "        self.triples = []\n",
    "        with open(os.path.join(data_path, triples_fn)) as f:\n",
    "            for line in f:\n",
    "                h, t, r = line.strip().split('\\t')\n",
    "                if h in self.struct_embeds and h in self.word_embeds and \\\n",
    "                        t in self.struct_embeds and t in self.word_embeds:\n",
    "                    # filtering triples missing an embedding\n",
    "                    self.triples.append((h, t, r))\n",
    "        self.triples_set = set(self.triples)\n",
    "\n",
    "        \"\"\"self.entities = []\n",
    "        with open(os.path.join(data_path, entities_fn)) as f:\n",
    "            for line in f:\n",
    "                eid, e = line.strip().split('\\t')\n",
    "                if e in self.struct_embeds and e in self.word_embeds:\n",
    "                    # filtering entities missing an embedding\n",
    "                    self.entities.append(e)\"\"\"\n",
    "\n",
    "        self.entities_names = []\n",
    "        with open(os.path.join(data_path, entities_names_fn)) as f:\n",
    "            for line in f:\n",
    "                eid, e = line.strip().split('\\t')\n",
    "                if e in self.struct_embeds and e in self.word_embeds:\n",
    "                    self.entities_names.append(e)\n",
    "\n",
    "        print(f'Number of triples: {len(self.triples)}\\nNumber of entities_names: {len(self.entities_names)}\\n')\n",
    "\n",
    "        self.transform = torch.tensor\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.triples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        pos_triple = self.triples[idx]\n",
    "        neg_triple_head = self.sample_negative_head(pos_triple)\n",
    "        neg_triple_tail = self.sample_negative_tail(pos_triple)\n",
    "\n",
    "        return (self.transform(self.struct_embeds[pos_triple[2]]),\n",
    "                self.transform(self.struct_embeds[pos_triple[0]]),\n",
    "                self.transform(self.struct_embeds[pos_triple[1]]),\n",
    "                self.transform(self.word_embeds[pos_triple[0]]),\n",
    "                self.transform(self.word_embeds[pos_triple[1]]),\n",
    "                self.transform(self.struct_embeds[neg_triple_head[0]]),\n",
    "                self.transform(self.struct_embeds[neg_triple_tail[1]]),\n",
    "                self.transform(self.word_embeds[neg_triple_head[0]]),\n",
    "                self.transform(self.word_embeds[neg_triple_tail[1]]))\n",
    "\n",
    "    def sample_negative_head(self, triple_to_corrupt):\n",
    "        for i in range(len(self.entities_names)):\n",
    "            index = np.random.randint(0, len(self.entities_names))\n",
    "            random_entity = self.entities_names[index]\n",
    "            if random_entity != triple_to_corrupt[0]:\n",
    "                negative_triple = (random_entity, triple_to_corrupt[1], triple_to_corrupt[2])\n",
    "                if negative_triple not in self.triples:\n",
    "                    return negative_triple\n",
    "        negative_triple = (triple_to_corrupt[1], triple_to_corrupt[1], triple_to_corrupt[2])\n",
    "        return negative_triple\n",
    "\n",
    "    def sample_negative_tail(self, triple_to_corrupt):\n",
    "        for i in range(len(self.entities_names)):\n",
    "            index = np.random.randint(0, len(self.entities_names))\n",
    "            random_entity = self.entities_names[index]\n",
    "            if random_entity != triple_to_corrupt[1]:\n",
    "                negative_triple = (triple_to_corrupt[0], random_entity, triple_to_corrupt[2])\n",
    "                if negative_triple not in self.triples:\n",
    "                    return negative_triple\n",
    "        negative_triple = (triple_to_corrupt[0], triple_to_corrupt[0], triple_to_corrupt[2])\n",
    "        return negative_triple\n",
    "\n",
    "\n",
    "class EmbeddingModel(nn.Module):\n",
    "\n",
    "    def __init__(self, struct_embed_size, text_embed_size, dropout_rate=0.5, out_size=100):\n",
    "        super(EmbeddingModel, self).__init__()\n",
    "\n",
    "        self.rel = nn.Linear(struct_embed_size, out_size)\n",
    "        self.fc_head_struct_pos = nn.Linear(struct_embed_size, out_size)\n",
    "        self.fc_head_struct_neg = nn.Linear(struct_embed_size, out_size)\n",
    "        self.fc_head_text_pos = nn.Linear(text_embed_size, out_size)\n",
    "        self.fc_head_text_neg = nn.Linear(text_embed_size, out_size)\n",
    "        self.fc_tail_struct_pos = nn.Linear(struct_embed_size, out_size)\n",
    "        self.fc_tail_struct_neg = nn.Linear(struct_embed_size, out_size)\n",
    "        self.fc_tail_text_pos = nn.Linear(text_embed_size, out_size)\n",
    "        self.fc_tail_text_neg = nn.Linear(text_embed_size, out_size)\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "\n",
    "    def forward(self, data):\n",
    "        rel, head_struct_pos, tail_struct_pos, head_text_pos, tail_text_pos, \\\n",
    "             head_struct_neg, tail_struct_neg, head_text_neg, tail_text_neg = data\n",
    "        rel = self.dropout(F.relu(self.rel(rel)))\n",
    "        hsp = self.dropout(F.relu(self.fc_head_struct_pos(head_struct_pos)))\n",
    "        htp = self.dropout(F.relu(self.fc_head_text_pos(head_text_pos)))\n",
    "        hsn = self.dropout(F.relu(self.fc_head_struct_neg(head_struct_neg)))\n",
    "        htn = self.dropout(F.relu(self.fc_head_text_neg(head_text_neg)))\n",
    "        tsp = self.dropout(F.relu(self.fc_tail_struct_pos(tail_struct_pos)))\n",
    "        ttp = self.dropout(F.relu(self.fc_tail_text_pos(tail_text_pos)))\n",
    "        tsn = self.dropout(F.relu(self.fc_tail_struct_neg(tail_struct_neg)))\n",
    "        ttn = self.dropout(F.relu(self.fc_tail_text_neg(tail_text_neg)))\n",
    "\n",
    "        return (rel, hsp, tsp, htp, ttp, hsn, tsn, htn, ttn)\n",
    "\n",
    "    def score(self, data):\n",
    "        rel, head_struct_pos, tail_struct_pos, head_text_pos, tail_text_pos, \\\n",
    "             head_struct_neg, tail_struct_neg, head_text_neg, tail_text_neg = data\n",
    "\n",
    "        # head\n",
    "        head_fs_pos = torch.sum(abs(head_struct_pos + rel - tail_struct_pos), 1, keepdim=True)\n",
    "        head_fs_neg = torch.sum(abs(head_struct_pos + rel - tail_struct_neg), 1, keepdim=True)\n",
    "\n",
    "        head_fts_pos = torch.sum(abs(head_text_pos + rel - tail_struct_pos), 1, keepdim=True)\n",
    "        head_fts_neg = torch.sum(abs(head_text_pos + rel - tail_struct_neg), 1, keepdim=True)\n",
    "\n",
    "        head_fst_pos = torch.sum(abs(head_struct_pos + rel - tail_text_pos), 1, keepdim=True)\n",
    "        head_fst_neg = torch.sum(abs(head_struct_pos + rel - tail_text_neg), 1, keepdim=True)\n",
    "\n",
    "        head_ft_pos = torch.sum(abs(head_text_pos + rel - tail_text_pos), 1, keepdim=True)\n",
    "        head_ft_neg = torch.sum(abs(head_text_pos + rel - tail_text_neg), 1, keepdim=True)\n",
    "\n",
    "        head_fadd_pos = torch.sum(abs((head_struct_pos + head_text_pos)\n",
    "                                      + rel - (tail_struct_pos + tail_text_pos)), 1, keepdim=True)\n",
    "        head_fadd_neg = torch.sum(abs((head_struct_pos + head_text_pos)\n",
    "                                      + rel - (tail_struct_neg + tail_text_neg)), 1, keepdim=True)\n",
    "\n",
    "        head_ftotal_pos = torch.sum(torch.cat([head_fs_pos, head_fts_pos, head_fst_pos,\n",
    "                                     head_ft_pos, head_fadd_pos]), 0, keepdim=False)\n",
    "        head_ftotal_neg = torch.sum(torch.cat([head_fs_neg, head_fts_neg, head_fst_neg,\n",
    "                                     head_ft_neg, head_fadd_neg]), 0, keepdim=False)\n",
    "\n",
    "        # tail\n",
    "        tail_fs_pos = torch.sum(abs(tail_struct_pos - rel - head_struct_pos), 1, keepdim=True)\n",
    "        tail_fs_neg = torch.sum(abs(tail_struct_pos - rel - head_struct_neg), 1, keepdim=True)\n",
    "\n",
    "        tail_fts_pos = torch.sum(abs(tail_text_pos - rel - head_struct_pos), 1, keepdim=True)\n",
    "        tail_fts_neg = torch.sum(abs(tail_text_pos - rel - head_struct_neg), 1, keepdim=True)\n",
    "\n",
    "        tail_fst_pos = torch.sum(abs(tail_struct_pos - rel - head_text_pos), 1, keepdim=True)\n",
    "        tail_fst_neg = torch.sum(abs(tail_struct_pos - rel - head_text_neg), 1, keepdim=True)\n",
    "\n",
    "        tail_ft_pos = torch.sum(abs(tail_text_pos - rel - head_text_pos), 1, keepdim=True)\n",
    "        tail_ft_neg = torch.sum(abs(tail_text_pos - rel - head_text_neg), 1, keepdim=True)\n",
    "\n",
    "        tail_fadd_pos = torch.sum(abs((tail_struct_pos + tail_text_pos)\n",
    "                                      - rel - (head_struct_pos + head_text_pos)), 1, keepdim=True)\n",
    "        tail_fadd_neg = torch.sum(abs((tail_struct_pos + tail_text_pos)\n",
    "                                      - rel - (head_struct_neg + head_text_neg)), 1, keepdim=True)\n",
    "\n",
    "        tail_ftotal_pos = torch.sum(torch.cat([tail_fs_pos, tail_fts_pos, tail_fst_pos,\n",
    "                                     tail_ft_pos, tail_fadd_pos]), 0, keepdim=False)\n",
    "        tail_ftotal_neg = torch.sum(torch.cat([tail_fs_neg, tail_fts_neg, tail_fst_neg,\n",
    "                                     tail_ft_neg, tail_fadd_neg]), 0, keepdim=False)\n",
    "\n",
    "        return head_ftotal_pos, head_ftotal_neg, tail_ftotal_pos, tail_ftotal_neg\n",
    "\n",
    "    def loss(self, margin, data):\n",
    "        # rel, hsp, tsp, htp, ttp, hsn, tsn, htn, ttn = data\n",
    "        score_head_pos, score_head_neg, score_tail_pos, score_tail_neg = self.score(data)\n",
    "        print(f'Score head pos: {score_head_pos}\\nScore head neg: {score_head_neg}\\nScore tail pos: {score_tail_pos}\\nScore tail neg:{score_tail_neg}')\n",
    "        print(f'loss_1: {margin - score_head_neg + score_head_pos}')\n",
    "        print(f'loss_2: {margin - score_tail_neg + score_tail_pos}')\n",
    "        loss_1 = torch.maximum(torch.tensor((0.)), margin - score_head_neg + score_head_pos)\n",
    "        loss_2 = torch.maximum(torch.tensor((0.)), margin - score_tail_neg + score_tail_pos)\n",
    "        # loss_1 = margin - score_head_neg + score_head_pos\n",
    "        # loss_2 = margin - score_tail_neg + score_tail_pos\n",
    "        return loss_1 + loss_2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1c58a83",
   "metadata": {},
   "source": [
    "#### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "0ec25b11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Number of triples: 547034\n",
      "Number of entities_names: 123951\n",
      "\n",
      "Starting epoch 1\n",
      "Score head pos: tensor([3431.1870], grad_fn=<SumBackward1>)\n",
      "Score head neg: tensor([3532.3328], grad_fn=<SumBackward1>)\n",
      "Score tail pos: tensor([3431.1870], grad_fn=<SumBackward1>)\n",
      "Score tail neg:tensor([3606.8984], grad_fn=<SumBackward1>)\n",
      "loss_1: tensor([-91.1458], grad_fn=<AddBackward0>)\n",
      "loss_2: tensor([-165.7114], grad_fn=<AddBackward0>)\n",
      "Loss: 0.0\n",
      "Score head pos: tensor([3426.9138], grad_fn=<SumBackward1>)\n",
      "Score head neg: tensor([3498.9270], grad_fn=<SumBackward1>)\n",
      "Score tail pos: tensor([3426.9138], grad_fn=<SumBackward1>)\n",
      "Score tail neg:tensor([3590.9128], grad_fn=<SumBackward1>)\n",
      "loss_1: tensor([-62.0132], grad_fn=<AddBackward0>)\n",
      "loss_2: tensor([-153.9990], grad_fn=<AddBackward0>)\n",
      "Loss: 0.0\n",
      "Score head pos: tensor([3402.5374], grad_fn=<SumBackward1>)\n",
      "Score head neg: tensor([3516.0481], grad_fn=<SumBackward1>)\n",
      "Score tail pos: tensor([3402.5371], grad_fn=<SumBackward1>)\n",
      "Score tail neg:tensor([3545.5022], grad_fn=<SumBackward1>)\n",
      "loss_1: tensor([-103.5107], grad_fn=<AddBackward0>)\n",
      "loss_2: tensor([-132.9651], grad_fn=<AddBackward0>)\n",
      "Loss: 0.0\n",
      "Score head pos: tensor([3458.8806], grad_fn=<SumBackward1>)\n",
      "Score head neg: tensor([3564.3154], grad_fn=<SumBackward1>)\n",
      "Score tail pos: tensor([3458.8806], grad_fn=<SumBackward1>)\n",
      "Score tail neg:tensor([3609.2590], grad_fn=<SumBackward1>)\n",
      "loss_1: tensor([-95.4348], grad_fn=<AddBackward0>)\n",
      "loss_2: tensor([-140.3784], grad_fn=<AddBackward0>)\n",
      "Loss: 0.0\n",
      "Score head pos: tensor([3428.6245], grad_fn=<SumBackward1>)\n",
      "Score head neg: tensor([3537.2761], grad_fn=<SumBackward1>)\n",
      "Score tail pos: tensor([3428.6245], grad_fn=<SumBackward1>)\n",
      "Score tail neg:tensor([3565.6375], grad_fn=<SumBackward1>)\n",
      "loss_1: tensor([-98.6516], grad_fn=<AddBackward0>)\n",
      "loss_2: tensor([-127.0129], grad_fn=<AddBackward0>)\n",
      "Loss: 0.0\n",
      "Score head pos: tensor([3458.3650], grad_fn=<SumBackward1>)\n",
      "Score head neg: tensor([3528.2080], grad_fn=<SumBackward1>)\n",
      "Score tail pos: tensor([3458.3650], grad_fn=<SumBackward1>)\n",
      "Score tail neg:tensor([3558.9392], grad_fn=<SumBackward1>)\n",
      "loss_1: tensor([-59.8430], grad_fn=<AddBackward0>)\n",
      "loss_2: tensor([-90.5742], grad_fn=<AddBackward0>)\n",
      "Loss: 0.0\n",
      "Score head pos: tensor([3397.6650], grad_fn=<SumBackward1>)\n",
      "Score head neg: tensor([3517.0837], grad_fn=<SumBackward1>)\n",
      "Score tail pos: tensor([3397.6650], grad_fn=<SumBackward1>)\n",
      "Score tail neg:tensor([3531.2327], grad_fn=<SumBackward1>)\n",
      "loss_1: tensor([-109.4187], grad_fn=<AddBackward0>)\n",
      "loss_2: tensor([-123.5676], grad_fn=<AddBackward0>)\n",
      "Loss: 0.0\n",
      "Score head pos: tensor([3437.6123], grad_fn=<SumBackward1>)\n",
      "Score head neg: tensor([3545.0359], grad_fn=<SumBackward1>)\n",
      "Score tail pos: tensor([3437.6118], grad_fn=<SumBackward1>)\n",
      "Score tail neg:tensor([3632.6113], grad_fn=<SumBackward1>)\n",
      "loss_1: tensor([-97.4236], grad_fn=<AddBackward0>)\n",
      "loss_2: tensor([-184.9995], grad_fn=<AddBackward0>)\n",
      "Loss: 0.0\n",
      "Score head pos: tensor([3367.4233], grad_fn=<SumBackward1>)\n",
      "Score head neg: tensor([3478.6340], grad_fn=<SumBackward1>)\n",
      "Score tail pos: tensor([3367.4229], grad_fn=<SumBackward1>)\n",
      "Score tail neg:tensor([3537.1716], grad_fn=<SumBackward1>)\n",
      "loss_1: tensor([-101.2107], grad_fn=<AddBackward0>)\n",
      "loss_2: tensor([-159.7488], grad_fn=<AddBackward0>)\n",
      "Loss: 0.0\n",
      "Loss after mini-batch     9: 0.000\n",
      "Score head pos: tensor([3454.1758], grad_fn=<SumBackward1>)\n",
      "Score head neg: tensor([3541.3115], grad_fn=<SumBackward1>)\n",
      "Score tail pos: tensor([3454.1758], grad_fn=<SumBackward1>)\n",
      "Score tail neg:tensor([3594.5793], grad_fn=<SumBackward1>)\n",
      "loss_1: tensor([-77.1357], grad_fn=<AddBackward0>)\n",
      "loss_2: tensor([-130.4036], grad_fn=<AddBackward0>)\n",
      "Loss: 0.0\n",
      "Score head pos: tensor([3416.8428], grad_fn=<SumBackward1>)\n",
      "Score head neg: tensor([3537.3284], grad_fn=<SumBackward1>)\n",
      "Score tail pos: tensor([3416.8430], grad_fn=<SumBackward1>)\n",
      "Score tail neg:tensor([3584.6597], grad_fn=<SumBackward1>)\n",
      "loss_1: tensor([-110.4856], grad_fn=<AddBackward0>)\n",
      "loss_2: tensor([-157.8167], grad_fn=<AddBackward0>)\n",
      "Loss: 0.0\n",
      "Score head pos: tensor([3425.0796], grad_fn=<SumBackward1>)\n",
      "Score head neg: tensor([3595.1050], grad_fn=<SumBackward1>)\n",
      "Score tail pos: tensor([3425.0796], grad_fn=<SumBackward1>)\n",
      "Score tail neg:tensor([3582.2278], grad_fn=<SumBackward1>)\n",
      "loss_1: tensor([-160.0254], grad_fn=<AddBackward0>)\n",
      "loss_2: tensor([-147.1482], grad_fn=<AddBackward0>)\n",
      "Loss: 0.0\n",
      "Score head pos: tensor([3427.4500], grad_fn=<SumBackward1>)\n",
      "Score head neg: tensor([3556.7957], grad_fn=<SumBackward1>)\n",
      "Score tail pos: tensor([3427.4500], grad_fn=<SumBackward1>)\n",
      "Score tail neg:tensor([3590.4331], grad_fn=<SumBackward1>)\n",
      "loss_1: tensor([-119.3457], grad_fn=<AddBackward0>)\n",
      "loss_2: tensor([-152.9832], grad_fn=<AddBackward0>)\n",
      "Loss: 0.0\n",
      "Score head pos: tensor([3434.7065], grad_fn=<SumBackward1>)\n",
      "Score head neg: tensor([3504.1392], grad_fn=<SumBackward1>)\n",
      "Score tail pos: tensor([3434.7065], grad_fn=<SumBackward1>)\n",
      "Score tail neg:tensor([3577.2874], grad_fn=<SumBackward1>)\n",
      "loss_1: tensor([-59.4326], grad_fn=<AddBackward0>)\n",
      "loss_2: tensor([-132.5808], grad_fn=<AddBackward0>)\n",
      "Loss: 0.0\n",
      "Score head pos: tensor([3395.2048], grad_fn=<SumBackward1>)\n",
      "Score head neg: tensor([3506.6431], grad_fn=<SumBackward1>)\n",
      "Score tail pos: tensor([3395.2048], grad_fn=<SumBackward1>)\n",
      "Score tail neg:tensor([3569.4751], grad_fn=<SumBackward1>)\n",
      "loss_1: tensor([-101.4382], grad_fn=<AddBackward0>)\n",
      "loss_2: tensor([-164.2703], grad_fn=<AddBackward0>)\n",
      "Loss: 0.0\n",
      "Score head pos: tensor([3503.4683], grad_fn=<SumBackward1>)\n",
      "Score head neg: tensor([3562.3296], grad_fn=<SumBackward1>)\n",
      "Score tail pos: tensor([3503.4685], grad_fn=<SumBackward1>)\n",
      "Score tail neg:tensor([3648.8577], grad_fn=<SumBackward1>)\n",
      "loss_1: tensor([-48.8613], grad_fn=<AddBackward0>)\n",
      "loss_2: tensor([-135.3892], grad_fn=<AddBackward0>)\n",
      "Loss: 0.0\n",
      "Score head pos: tensor([3377.3257], grad_fn=<SumBackward1>)\n",
      "Score head neg: tensor([3490.9253], grad_fn=<SumBackward1>)\n",
      "Score tail pos: tensor([3377.3252], grad_fn=<SumBackward1>)\n",
      "Score tail neg:tensor([3559.7397], grad_fn=<SumBackward1>)\n",
      "loss_1: tensor([-103.5996], grad_fn=<AddBackward0>)\n",
      "loss_2: tensor([-172.4146], grad_fn=<AddBackward0>)\n",
      "Loss: 0.0\n",
      "Score head pos: tensor([3436.9805], grad_fn=<SumBackward1>)\n",
      "Score head neg: tensor([3543.3342], grad_fn=<SumBackward1>)\n",
      "Score tail pos: tensor([3436.9805], grad_fn=<SumBackward1>)\n",
      "Score tail neg:tensor([3574.6736], grad_fn=<SumBackward1>)\n",
      "loss_1: tensor([-96.3538], grad_fn=<AddBackward0>)\n",
      "loss_2: tensor([-127.6931], grad_fn=<AddBackward0>)\n",
      "Loss: 0.0\n",
      "Score head pos: tensor([3423.8235], grad_fn=<SumBackward1>)\n",
      "Score head neg: tensor([3507.3669], grad_fn=<SumBackward1>)\n",
      "Score tail pos: tensor([3423.8232], grad_fn=<SumBackward1>)\n",
      "Score tail neg:tensor([3618.0962], grad_fn=<SumBackward1>)\n",
      "loss_1: tensor([-73.5435], grad_fn=<AddBackward0>)\n",
      "loss_2: tensor([-184.2729], grad_fn=<AddBackward0>)\n",
      "Loss: 0.0\n",
      "Loss after mini-batch    19: 0.000\n",
      "Score head pos: tensor([3399.6582], grad_fn=<SumBackward1>)\n",
      "Score head neg: tensor([3500.8401], grad_fn=<SumBackward1>)\n",
      "Score tail pos: tensor([3399.6580], grad_fn=<SumBackward1>)\n",
      "Score tail neg:tensor([3533.2893], grad_fn=<SumBackward1>)\n",
      "loss_1: tensor([-91.1819], grad_fn=<AddBackward0>)\n",
      "loss_2: tensor([-123.6313], grad_fn=<AddBackward0>)\n",
      "Loss: 0.0\n",
      "Score head pos: tensor([3502.3799], grad_fn=<SumBackward1>)\n",
      "Score head neg: tensor([3673.9062], grad_fn=<SumBackward1>)\n",
      "Score tail pos: tensor([3502.3796], grad_fn=<SumBackward1>)\n",
      "Score tail neg:tensor([3642.9631], grad_fn=<SumBackward1>)\n",
      "loss_1: tensor([-161.5264], grad_fn=<AddBackward0>)\n",
      "loss_2: tensor([-130.5835], grad_fn=<AddBackward0>)\n",
      "Loss: 0.0\n",
      "Score head pos: tensor([3437.4548], grad_fn=<SumBackward1>)\n",
      "Score head neg: tensor([3514.1331], grad_fn=<SumBackward1>)\n",
      "Score tail pos: tensor([3437.4553], grad_fn=<SumBackward1>)\n",
      "Score tail neg:tensor([3536.0166], grad_fn=<SumBackward1>)\n",
      "loss_1: tensor([-66.6782], grad_fn=<AddBackward0>)\n",
      "loss_2: tensor([-88.5613], grad_fn=<AddBackward0>)\n",
      "Loss: 0.0\n",
      "Score head pos: tensor([3358.0752], grad_fn=<SumBackward1>)\n",
      "Score head neg: tensor([3486.4358], grad_fn=<SumBackward1>)\n",
      "Score tail pos: tensor([3358.0752], grad_fn=<SumBackward1>)\n",
      "Score tail neg:tensor([3537.1572], grad_fn=<SumBackward1>)\n",
      "loss_1: tensor([-118.3606], grad_fn=<AddBackward0>)\n",
      "loss_2: tensor([-169.0820], grad_fn=<AddBackward0>)\n",
      "Loss: 0.0\n",
      "Score head pos: tensor([3432.7129], grad_fn=<SumBackward1>)\n",
      "Score head neg: tensor([3560.4233], grad_fn=<SumBackward1>)\n",
      "Score tail pos: tensor([3432.7129], grad_fn=<SumBackward1>)\n",
      "Score tail neg:tensor([3604.1580], grad_fn=<SumBackward1>)\n",
      "loss_1: tensor([-117.7104], grad_fn=<AddBackward0>)\n",
      "loss_2: tensor([-161.4451], grad_fn=<AddBackward0>)\n",
      "Loss: 0.0\n",
      "Score head pos: tensor([3433.6565], grad_fn=<SumBackward1>)\n",
      "Score head neg: tensor([3575.1516], grad_fn=<SumBackward1>)\n",
      "Score tail pos: tensor([3433.6570], grad_fn=<SumBackward1>)\n",
      "Score tail neg:tensor([3553.9712], grad_fn=<SumBackward1>)\n",
      "loss_1: tensor([-131.4951], grad_fn=<AddBackward0>)\n",
      "loss_2: tensor([-110.3142], grad_fn=<AddBackward0>)\n",
      "Loss: 0.0\n",
      "Score head pos: tensor([3475.2197], grad_fn=<SumBackward1>)\n",
      "Score head neg: tensor([3587.9104], grad_fn=<SumBackward1>)\n",
      "Score tail pos: tensor([3475.2197], grad_fn=<SumBackward1>)\n",
      "Score tail neg:tensor([3625.4771], grad_fn=<SumBackward1>)\n",
      "loss_1: tensor([-102.6907], grad_fn=<AddBackward0>)\n",
      "loss_2: tensor([-140.2573], grad_fn=<AddBackward0>)\n",
      "Loss: 0.0\n",
      "Score head pos: tensor([3394.7144], grad_fn=<SumBackward1>)\n",
      "Score head neg: tensor([3493.9893], grad_fn=<SumBackward1>)\n",
      "Score tail pos: tensor([3394.7144], grad_fn=<SumBackward1>)\n",
      "Score tail neg:tensor([3543.0562], grad_fn=<SumBackward1>)\n",
      "loss_1: tensor([-89.2749], grad_fn=<AddBackward0>)\n",
      "loss_2: tensor([-138.3418], grad_fn=<AddBackward0>)\n",
      "Loss: 0.0\n",
      "Score head pos: tensor([3486.0732], grad_fn=<SumBackward1>)\n",
      "Score head neg: tensor([3580.4963], grad_fn=<SumBackward1>)\n",
      "Score tail pos: tensor([3486.0735], grad_fn=<SumBackward1>)\n",
      "Score tail neg:tensor([3644.4355], grad_fn=<SumBackward1>)\n",
      "loss_1: tensor([-84.4231], grad_fn=<AddBackward0>)\n",
      "loss_2: tensor([-148.3621], grad_fn=<AddBackward0>)\n",
      "Loss: 0.0\n",
      "Score head pos: tensor([3415.3135], grad_fn=<SumBackward1>)\n",
      "Score head neg: tensor([3535.9844], grad_fn=<SumBackward1>)\n",
      "Score tail pos: tensor([3415.3135], grad_fn=<SumBackward1>)\n",
      "Score tail neg:tensor([3611.7935], grad_fn=<SumBackward1>)\n",
      "loss_1: tensor([-110.6709], grad_fn=<AddBackward0>)\n",
      "loss_2: tensor([-186.4800], grad_fn=<AddBackward0>)\n",
      "Loss: 0.0\n",
      "Loss after mini-batch    29: 0.000\n",
      "Score head pos: tensor([3427.4766], grad_fn=<SumBackward1>)\n",
      "Score head neg: tensor([3526.3743], grad_fn=<SumBackward1>)\n",
      "Score tail pos: tensor([3427.4763], grad_fn=<SumBackward1>)\n",
      "Score tail neg:tensor([3564.6108], grad_fn=<SumBackward1>)\n",
      "loss_1: tensor([-88.8977], grad_fn=<AddBackward0>)\n",
      "loss_2: tensor([-127.1345], grad_fn=<AddBackward0>)\n",
      "Loss: 0.0\n",
      "Score head pos: tensor([3419.5571], grad_fn=<SumBackward1>)\n",
      "Score head neg: tensor([3541.3809], grad_fn=<SumBackward1>)\n",
      "Score tail pos: tensor([3419.5571], grad_fn=<SumBackward1>)\n",
      "Score tail neg:tensor([3576.8733], grad_fn=<SumBackward1>)\n",
      "loss_1: tensor([-111.8237], grad_fn=<AddBackward0>)\n",
      "loss_2: tensor([-147.3162], grad_fn=<AddBackward0>)\n",
      "Loss: 0.0\n",
      "Score head pos: tensor([3487.0825], grad_fn=<SumBackward1>)\n",
      "Score head neg: tensor([3562.0432], grad_fn=<SumBackward1>)\n",
      "Score tail pos: tensor([3487.0823], grad_fn=<SumBackward1>)\n",
      "Score tail neg:tensor([3665.1123], grad_fn=<SumBackward1>)\n",
      "loss_1: tensor([-64.9607], grad_fn=<AddBackward0>)\n",
      "loss_2: tensor([-168.0300], grad_fn=<AddBackward0>)\n",
      "Loss: 0.0\n",
      "Score head pos: tensor([3416.1265], grad_fn=<SumBackward1>)\n",
      "Score head neg: tensor([3498.4141], grad_fn=<SumBackward1>)\n",
      "Score tail pos: tensor([3416.1267], grad_fn=<SumBackward1>)\n",
      "Score tail neg:tensor([3545.1309], grad_fn=<SumBackward1>)\n",
      "loss_1: tensor([-72.2876], grad_fn=<AddBackward0>)\n",
      "loss_2: tensor([-119.0042], grad_fn=<AddBackward0>)\n",
      "Loss: 0.0\n",
      "Score head pos: tensor([3406.2708], grad_fn=<SumBackward1>)\n",
      "Score head neg: tensor([3557.0061], grad_fn=<SumBackward1>)\n",
      "Score tail pos: tensor([3406.2708], grad_fn=<SumBackward1>)\n",
      "Score tail neg:tensor([3603.2163], grad_fn=<SumBackward1>)\n",
      "loss_1: tensor([-140.7354], grad_fn=<AddBackward0>)\n",
      "loss_2: tensor([-186.9456], grad_fn=<AddBackward0>)\n",
      "Loss: 0.0\n",
      "Score head pos: tensor([3395.6235], grad_fn=<SumBackward1>)\n",
      "Score head neg: tensor([3472.4185], grad_fn=<SumBackward1>)\n",
      "Score tail pos: tensor([3395.6230], grad_fn=<SumBackward1>)\n",
      "Score tail neg:tensor([3539.1631], grad_fn=<SumBackward1>)\n",
      "loss_1: tensor([-66.7949], grad_fn=<AddBackward0>)\n",
      "loss_2: tensor([-133.5400], grad_fn=<AddBackward0>)\n",
      "Loss: 0.0\n",
      "Score head pos: tensor([3422.8394], grad_fn=<SumBackward1>)\n",
      "Score head neg: tensor([3526.7537], grad_fn=<SumBackward1>)\n",
      "Score tail pos: tensor([3422.8398], grad_fn=<SumBackward1>)\n",
      "Score tail neg:tensor([3573.3052], grad_fn=<SumBackward1>)\n",
      "loss_1: tensor([-93.9143], grad_fn=<AddBackward0>)\n",
      "loss_2: tensor([-140.4653], grad_fn=<AddBackward0>)\n",
      "Loss: 0.0\n",
      "Score head pos: tensor([3462.9575], grad_fn=<SumBackward1>)\n",
      "Score head neg: tensor([3561.3032], grad_fn=<SumBackward1>)\n",
      "Score tail pos: tensor([3462.9578], grad_fn=<SumBackward1>)\n",
      "Score tail neg:tensor([3577.8794], grad_fn=<SumBackward1>)\n",
      "loss_1: tensor([-88.3457], grad_fn=<AddBackward0>)\n",
      "loss_2: tensor([-104.9216], grad_fn=<AddBackward0>)\n",
      "Loss: 0.0\n",
      "Score head pos: tensor([3471.4373], grad_fn=<SumBackward1>)\n",
      "Score head neg: tensor([3552.0049], grad_fn=<SumBackward1>)\n",
      "Score tail pos: tensor([3471.4373], grad_fn=<SumBackward1>)\n",
      "Score tail neg:tensor([3666.9355], grad_fn=<SumBackward1>)\n",
      "loss_1: tensor([-70.5676], grad_fn=<AddBackward0>)\n",
      "loss_2: tensor([-185.4983], grad_fn=<AddBackward0>)\n",
      "Loss: 0.0\n",
      "Score head pos: tensor([3414.7031], grad_fn=<SumBackward1>)\n",
      "Score head neg: tensor([3508.1794], grad_fn=<SumBackward1>)\n",
      "Score tail pos: tensor([3414.7031], grad_fn=<SumBackward1>)\n",
      "Score tail neg:tensor([3539.8875], grad_fn=<SumBackward1>)\n",
      "loss_1: tensor([-83.4763], grad_fn=<AddBackward0>)\n",
      "loss_2: tensor([-115.1843], grad_fn=<AddBackward0>)\n",
      "Loss: 0.0\n",
      "Loss after mini-batch    39: 0.000\n",
      "Score head pos: tensor([3454.3745], grad_fn=<SumBackward1>)\n",
      "Score head neg: tensor([3543.8035], grad_fn=<SumBackward1>)\n",
      "Score tail pos: tensor([3454.3745], grad_fn=<SumBackward1>)\n",
      "Score tail neg:tensor([3558.0767], grad_fn=<SumBackward1>)\n",
      "loss_1: tensor([-79.4290], grad_fn=<AddBackward0>)\n",
      "loss_2: tensor([-93.7021], grad_fn=<AddBackward0>)\n",
      "Loss: 0.0\n",
      "Score head pos: tensor([3389.0935], grad_fn=<SumBackward1>)\n",
      "Score head neg: tensor([3496.1604], grad_fn=<SumBackward1>)\n",
      "Score tail pos: tensor([3389.0933], grad_fn=<SumBackward1>)\n",
      "Score tail neg:tensor([3550.1160], grad_fn=<SumBackward1>)\n",
      "loss_1: tensor([-97.0669], grad_fn=<AddBackward0>)\n",
      "loss_2: tensor([-151.0227], grad_fn=<AddBackward0>)\n",
      "Loss: 0.0\n",
      "Score head pos: tensor([3408.1143], grad_fn=<SumBackward1>)\n",
      "Score head neg: tensor([3500.6160], grad_fn=<SumBackward1>)\n",
      "Score tail pos: tensor([3408.1140], grad_fn=<SumBackward1>)\n",
      "Score tail neg:tensor([3580.2815], grad_fn=<SumBackward1>)\n",
      "loss_1: tensor([-82.5017], grad_fn=<AddBackward0>)\n",
      "loss_2: tensor([-162.1675], grad_fn=<AddBackward0>)\n",
      "Loss: 0.0\n",
      "Score head pos: tensor([3391.9414], grad_fn=<SumBackward1>)\n",
      "Score head neg: tensor([3510.5649], grad_fn=<SumBackward1>)\n",
      "Score tail pos: tensor([3391.9414], grad_fn=<SumBackward1>)\n",
      "Score tail neg:tensor([3558.1553], grad_fn=<SumBackward1>)\n",
      "loss_1: tensor([-108.6235], grad_fn=<AddBackward0>)\n",
      "loss_2: tensor([-156.2139], grad_fn=<AddBackward0>)\n",
      "Loss: 0.0\n",
      "Score head pos: tensor([3413.7607], grad_fn=<SumBackward1>)\n",
      "Score head neg: tensor([3467.8926], grad_fn=<SumBackward1>)\n",
      "Score tail pos: tensor([3413.7607], grad_fn=<SumBackward1>)\n",
      "Score tail neg:tensor([3582.5835], grad_fn=<SumBackward1>)\n",
      "loss_1: tensor([-44.1318], grad_fn=<AddBackward0>)\n",
      "loss_2: tensor([-158.8228], grad_fn=<AddBackward0>)\n",
      "Loss: 0.0\n",
      "Score head pos: tensor([3478.1689], grad_fn=<SumBackward1>)\n",
      "Score head neg: tensor([3498.3149], grad_fn=<SumBackward1>)\n",
      "Score tail pos: tensor([3478.1689], grad_fn=<SumBackward1>)\n",
      "Score tail neg:tensor([3617.2190], grad_fn=<SumBackward1>)\n",
      "loss_1: tensor([-10.1460], grad_fn=<AddBackward0>)\n",
      "loss_2: tensor([-129.0500], grad_fn=<AddBackward0>)\n",
      "Loss: 0.0\n",
      "Score head pos: tensor([3440.1262], grad_fn=<SumBackward1>)\n",
      "Score head neg: tensor([3542.8042], grad_fn=<SumBackward1>)\n",
      "Score tail pos: tensor([3440.1265], grad_fn=<SumBackward1>)\n",
      "Score tail neg:tensor([3640.5078], grad_fn=<SumBackward1>)\n",
      "loss_1: tensor([-92.6780], grad_fn=<AddBackward0>)\n",
      "loss_2: tensor([-190.3813], grad_fn=<AddBackward0>)\n",
      "Loss: 0.0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_2214/2258924004.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;31m# Iterate over the DataLoader for training data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0;31m# Get inputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    519\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    520\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 521\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    522\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    523\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    559\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    560\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 561\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    562\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    563\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_2214/2953811672.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     43\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[0mpos_triple\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtriples\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m         \u001b[0mneg_triple_head\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample_negative_head\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpos_triple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m         \u001b[0mneg_triple_tail\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample_negative_tail\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpos_triple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_2214/2953811672.py\u001b[0m in \u001b[0;36msample_negative_head\u001b[0;34m(self, triple_to_corrupt)\u001b[0m\n\u001b[1;32m     62\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mrandom_entity\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mtriple_to_corrupt\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m                 \u001b[0mnegative_triple\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mrandom_entity\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtriple_to_corrupt\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtriple_to_corrupt\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m                 \u001b[0;32mif\u001b[0m \u001b[0mnegative_triple\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtriples\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mnegative_triple\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m         \u001b[0mnegative_triple\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtriple_to_corrupt\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtriple_to_corrupt\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtriple_to_corrupt\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "param = parse_args(args=['--data_dir', '../persistent/data',\n",
    "                         '--entities_embed_fn', 'entity_500.npy',\n",
    "                         '--relations_embed_fn', 'relation_500.npy',\n",
    "                         '--struct_embed_size', '100',\n",
    "                         '--text_embed_size', '300',\n",
    "                         '--display_loss_step', '10',\n",
    "                         '--margin', '10'])\n",
    "\n",
    "# Get cpu or gpu device for training.\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using {device} device\")\n",
    "\n",
    "# Set fixed random number seed\n",
    "torch.manual_seed(28062022)\n",
    "\n",
    "training_data = TrainDataSet(param.data_dir, param.onto_dir,\n",
    "                             param.struct_embeds_fn, param.word_embeds_fn,\n",
    "                             param.triples_fn, param.entities_fn,\n",
    "                             param.entities_names_fn)\n",
    "train_dataloader = DataLoader(training_data, batch_size=param.batch_size, shuffle=True)\n",
    "\n",
    "model = EmbeddingModel(param.struct_embed_size, param.text_embed_size).to(device)\n",
    "\n",
    "# loss_function\n",
    "\n",
    "current_learning_rate = param.initial_learning_rate\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=current_learning_rate)\n",
    "\n",
    "initial_valid_loss = 100\n",
    "\n",
    "training_losses = []\n",
    "\n",
    "# Run the training loop\n",
    "for epoch in range(param.training_epochs):\n",
    "\n",
    "    # Print epoch\n",
    "    print(f'Starting epoch {epoch+1}')\n",
    "\n",
    "    # Set current loss value\n",
    "    current_loss = 0.0\n",
    "\n",
    "    # Iterate over the DataLoader for training data\n",
    "    for i, data in enumerate(train_dataloader, 1):\n",
    "\n",
    "        # Get inputs\n",
    "        for d in data:\n",
    "            d.to(device)\n",
    "\n",
    "        # Zero the gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Perform forward pass\n",
    "        outputs = model(data)\n",
    "\n",
    "        # Compute loss\n",
    "        loss = model.loss(param.margin, outputs)\n",
    "        training_losses.append(loss)\n",
    "\n",
    "        # Perform backward pass\n",
    "        loss.backward()\n",
    "\n",
    "        # Perform optimization\n",
    "        optimizer.step()\n",
    "\n",
    "        # Print statistics\n",
    "        print(f'Loss: {loss.item()}')\n",
    "        current_loss += loss.item()\n",
    "        if i % param.display_loss_step == param.display_loss_step-1:\n",
    "            print('Loss after mini-batch %5d: %.3f' % (i, current_loss / param.display_loss_step))\n",
    "            current_loss = 0.0\n",
    "\n",
    "# Extracting and saving the embeddings\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
